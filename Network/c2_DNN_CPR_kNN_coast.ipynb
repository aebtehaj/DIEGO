{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Training d-DNN and e-DNNs using CPR coincidences over the coast**"
      ],
      "metadata": {
        "id": "cqRl6LEjd2v1"
      },
      "id": "cqRl6LEjd2v1"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "LIWceWLRd5Yk"
      },
      "id": "LIWceWLRd5Yk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Data**"
      ],
      "metadata": {
        "id": "zpVkki_AfAoU"
      },
      "id": "zpVkki_AfAoU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Data Loading and Organizing**"
      ],
      "metadata": {
        "id": "WKqc5RooeWjr"
      },
      "id": "WKqc5RooeWjr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdJ-2F0rd0L4",
        "outputId": "0ec3b320-282c-43c2-b4f5-d6e23b05109d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['X_coast_trn_detection',\n",
              " 'X_coast_trn_retrieval',\n",
              " 'X_coast_tst_detection',\n",
              " 'X_coast_tst_retrieval',\n",
              " 'y_coast_trn_detection',\n",
              " 'y_coast_trn_retrieval',\n",
              " 'y_coast_tst_detection',\n",
              " 'y_coast_tst_retrieval']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f = h5py.File('Data/Dictionaries/Dic_CPR_coast.mat','r')\n",
        "list(f.keys())"
      ],
      "id": "FdJ-2F0rd0L4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dq_QgBad0L5"
      },
      "outputs": [],
      "source": [
        "X1_trn_detection = np.transpose(f['X_coast_trn_detection'])\n",
        "y_trn_detection = np.transpose(f['y_coast_trn_detection'])\n",
        "X1_tst_detection = np.transpose(f['X_coast_tst_detection'])\n",
        "y_tst_detection = np.transpose(f['y_coast_tst_detection'])\n",
        "\n",
        "X1_trn_retrieval = np.transpose(f['X_coast_trn_retrieval'])\n",
        "y_trn_retrieval = np.transpose(f['y_coast_trn_retrieval'])\n",
        "X1_tst_retrieval = np.transpose(f['X_coast_tst_retrieval'])\n",
        "y_tst_retrieval = np.transpose(f['y_coast_tst_retrieval'])"
      ],
      "id": "_dq_QgBad0L5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtAH1umGd0L5"
      },
      "outputs": [],
      "source": [
        "X1_trn_detection.astype('float64')\n",
        "X1_tst_detection.astype('float64')\n",
        "y_trn_detection.astype('int64')\n",
        "y_tst_detection.astype('int64')\n",
        "\n",
        "X1_trn_retrieval.astype('float64')\n",
        "X1_tst_retrieval.astype('float64')\n",
        "y_trn_retrieval.astype('float64')\n",
        "y_tst_retrieval.astype('float64');"
      ],
      "id": "MtAH1umGd0L5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Data Normalizing**\n",
        "\n",
        "The input features for training have different ranges and units, therefore they need to be scaled to make the flow of the gradient decsent smooth and help the algorithm quickly reaches the optimal point of the cost function. Without scaling features, the algorithm may be biased toward those features which have larger magnitues. We used the following standardization:\n",
        "\n",
        "$X_i^{\\prime} = \\frac{X_i - \\mu}{σ}$\n",
        "\n",
        "In the above equation $X_i^{\\prime}$ is the scaled feature, $μ$ is the mean, and $σ$ is the standard deviation of the feature. In the next cell, we implement this scaling for the data sets."
      ],
      "metadata": {
        "id": "9SyfHg0GfFzD"
      },
      "id": "9SyfHg0GfFzD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBqtFUjDd0L6"
      },
      "outputs": [],
      "source": [
        "mean_detection = X1_trn_detection.mean(axis=0)\n",
        "X2_trn_detection = X1_trn_detection-mean_detection\n",
        "std_detection = X1_trn_detection.std(axis=0)\n",
        "X_trn_detection = X2_trn_detection/std_detection\n",
        "X2_tst_detection = X1_tst_detection-mean_detection\n",
        "X_tst_detection = X2_tst_detection/std_detection\n",
        "\n",
        "mean_retrieval = X1_trn_retrieval.mean(axis=0)\n",
        "X2_trn_retrieval = X1_trn_retrieval-mean_retrieval\n",
        "std_retrieval = X1_trn_retrieval.std(axis=0)\n",
        "X_trn_retrieval = X2_trn_retrieval/std_retrieval\n",
        "X2_tst_retrieval = X1_tst_retrieval-mean_retrieval\n",
        "X_tst_retrieval = X2_tst_retrieval/std_retrieval"
      ],
      "id": "KBqtFUjDd0L6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show that the labels are balanced in the training and test set, the number of snowfall, rainfall and no precipitation lables is printed in the next cell."
      ],
      "metadata": {
        "id": "LCoUmi8gfvPL"
      },
      "id": "LCoUmi8gfvPL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GmO7cQ3d0L6",
        "outputId": "345e06ab-ddea-4486-9d5f-d0a8b7f2a6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***Training Dataset:\n",
            "\n",
            "Num. of snowfall: 1929\n",
            "Num. of rainfall: 1978\n",
            "Num. of clear-sky: 3882\n",
            "\n",
            "***Testing Dataset:\n",
            "\n",
            "Num. of snowfall: 848\n",
            "Num. of rainfall: 807\n",
            "Num. of clear-sky: 1683\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('***Training Dataset:\\n')\n",
        "\n",
        "n_snow=1\n",
        "n_rain=1\n",
        "n_clear=1  \n",
        "t_train = np.zeros([y_trn_detection.shape[0],1])\n",
        "\n",
        "for i in range(len(X_trn_detection)):\n",
        "    label = y_trn_detection[i]\n",
        "    if label==5:\n",
        "        n_snow+=1\n",
        "        t_train[i]=2\n",
        "    if label==3:\n",
        "        n_rain+=1  \n",
        "        t_train[i]=1\n",
        "    if label==0:\n",
        "        n_clear+=1\n",
        "        t_train[i]=0\n",
        "        \n",
        "print('Num. of snowfall:',n_snow)\n",
        "print('Num. of rainfall:',n_rain)\n",
        "print('Num. of clear-sky:',n_clear)\n",
        "\n",
        "print('\\n***Testing Dataset:\\n')\n",
        "\n",
        "n_snow=1\n",
        "n_rain=1\n",
        "n_clear=1  \n",
        "t_test = np.zeros([y_tst_detection.shape[0],1])\n",
        "\n",
        "for i in range(len(X_tst_detection)):\n",
        "    label = y_tst_detection[i]\n",
        "    if label==5:\n",
        "        n_snow+=1\n",
        "        t_test[i]=2\n",
        "    if label==3:\n",
        "        n_rain+=1  \n",
        "        t_test[i]=1\n",
        "    if label==0:\n",
        "        n_clear+=1\n",
        "        t_test[i]=0\n",
        "        \n",
        "print('Num. of snowfall:',n_snow)\n",
        "print('Num. of rainfall:',n_rain)\n",
        "print('Num. of clear-sky:',n_clear)"
      ],
      "id": "4GmO7cQ3d0L6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fms57Lhd0L6"
      },
      "outputs": [],
      "source": [
        "# Change the labels to catagorical\n",
        "t_trn_detection = to_categorical(t_train)\n",
        "t_tst_detection = to_categorical(t_test)"
      ],
      "id": "3Fms57Lhd0L6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Training the networks**"
      ],
      "metadata": {
        "id": "HqH-HbxRfzvr"
      },
      "id": "HqH-HbxRfzvr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REniJ7wMd0L7"
      },
      "source": [
        "### **2.1 - Detection Network (d-DNN)**"
      ],
      "id": "REniJ7wMd0L7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7eH-IUBd0L7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import layers, Sequential"
      ],
      "id": "F7eH-IUBd0L7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRcQhy46d0L7"
      },
      "outputs": [],
      "source": [
        "# Defining the architecture of the d-DNN network which has 6 layers and 30 hidden units in each layer.\n",
        "\n",
        "# Parameters\n",
        "hidden_units = 30\n",
        "dropout = 0\n",
        "\n",
        "# Detection Module\n",
        "model_detection = Sequential()\n",
        "\n",
        "model_detection.add(Dense(hidden_units))\n",
        "model_detection.add(Activation('relu'))\n",
        "model_detection.add(Dropout(dropout))\n",
        "\n",
        "model_detection.add(Dense(hidden_units))\n",
        "model_detection.add(Activation('relu'))\n",
        "model_detection.add(Dropout(dropout))\n",
        "\n",
        "model_detection.add(Dense(hidden_units))\n",
        "model_detection.add(Activation('relu'))\n",
        "model_detection.add(Dropout(dropout))\n",
        "\n",
        "model_detection.add(Dense(hidden_units))\n",
        "model_detection.add(Activation('relu'))\n",
        "model_detection.add(Dropout(dropout))\n",
        "\n",
        "model_detection.add(Dense(hidden_units))\n",
        "model_detection.add(Activation('relu'))\n",
        "model_detection.add(Dropout(dropout))\n",
        "\n",
        "model_detection.add(Dense(hidden_units))\n",
        "model_detection.add(Activation('relu'))\n",
        "model_detection.add(Dropout(dropout))\n",
        "\n",
        "model_detection.add(Dense(3))\n",
        "model_detection.add(Activation('softmax'))"
      ],
      "id": "TRcQhy46d0L7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GylXAFGjd0L8"
      },
      "outputs": [],
      "source": [
        "# Compiling the model by defining the loss function and learning rate.\n",
        "model_detection.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics= [tf.keras.metrics.Recall()])"
      ],
      "id": "GylXAFGjd0L8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnVAQYIVd0L8"
      },
      "outputs": [],
      "source": [
        "# Defining the callback list for early stoping and saving the model.\n",
        "from tensorflow import keras\n",
        "callbacks_list = [\n",
        "     keras.callbacks.EarlyStopping(\n",
        "     monitor=\"val_loss\",\n",
        "     patience=20,),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"checkpoint_path.keras\",\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    )    \n",
        "]"
      ],
      "id": "HnVAQYIVd0L8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlqhyIv9d0L8",
        "outputId": "6afc92f3-d968-4f60-805f-46b120fc068a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting DNN (Detection Module):\n",
            "\n",
            "Epoch 1/300\n",
            "156/156 [==============================] - 0s 2ms/step - loss: 1.0192 - recall: 0.0389 - val_loss: 0.9190 - val_recall: 0.1746\n",
            "Epoch 2/300\n",
            "156/156 [==============================] - 0s 940us/step - loss: 0.8288 - recall: 0.2728 - val_loss: 0.7160 - val_recall: 0.4050\n",
            "Epoch 3/300\n",
            "156/156 [==============================] - 0s 892us/step - loss: 0.6473 - recall: 0.5156 - val_loss: 0.5426 - val_recall: 0.6566\n",
            "Epoch 4/300\n",
            "156/156 [==============================] - 0s 891us/step - loss: 0.4737 - recall: 0.7506 - val_loss: 0.3881 - val_recall: 0.8614\n",
            "Epoch 5/300\n",
            "156/156 [==============================] - 0s 909us/step - loss: 0.3441 - recall: 0.8744 - val_loss: 0.3033 - val_recall: 0.8935\n",
            "Epoch 6/300\n",
            "156/156 [==============================] - 0s 911us/step - loss: 0.2839 - recall: 0.8964 - val_loss: 0.2683 - val_recall: 0.9121\n",
            "Epoch 7/300\n",
            "156/156 [==============================] - 0s 904us/step - loss: 0.2568 - recall: 0.9049 - val_loss: 0.2514 - val_recall: 0.9159\n",
            "Epoch 8/300\n",
            "156/156 [==============================] - 0s 910us/step - loss: 0.2418 - recall: 0.9102 - val_loss: 0.2401 - val_recall: 0.9185\n",
            "Epoch 9/300\n",
            "156/156 [==============================] - 0s 912us/step - loss: 0.2320 - recall: 0.9131 - val_loss: 0.2313 - val_recall: 0.9178\n",
            "Epoch 10/300\n",
            "156/156 [==============================] - 0s 910us/step - loss: 0.2243 - recall: 0.9183 - val_loss: 0.2243 - val_recall: 0.9223\n",
            "Epoch 11/300\n",
            "156/156 [==============================] - 0s 911us/step - loss: 0.2184 - recall: 0.9215 - val_loss: 0.2196 - val_recall: 0.9223\n",
            "Epoch 12/300\n",
            "156/156 [==============================] - 0s 963us/step - loss: 0.2137 - recall: 0.9204 - val_loss: 0.2149 - val_recall: 0.9198\n",
            "Epoch 13/300\n",
            "156/156 [==============================] - 0s 933us/step - loss: 0.2105 - recall: 0.9202 - val_loss: 0.2118 - val_recall: 0.9211\n",
            "Epoch 14/300\n",
            "156/156 [==============================] - 0s 917us/step - loss: 0.2069 - recall: 0.9220 - val_loss: 0.2089 - val_recall: 0.9243\n",
            "Epoch 15/300\n",
            "156/156 [==============================] - 0s 900us/step - loss: 0.2035 - recall: 0.9228 - val_loss: 0.2057 - val_recall: 0.9243\n",
            "Epoch 16/300\n",
            "156/156 [==============================] - 0s 913us/step - loss: 0.2005 - recall: 0.9247 - val_loss: 0.2025 - val_recall: 0.9249\n",
            "Epoch 17/300\n",
            "156/156 [==============================] - 0s 900us/step - loss: 0.1977 - recall: 0.9257 - val_loss: 0.2009 - val_recall: 0.9255\n",
            "Epoch 18/300\n",
            "156/156 [==============================] - 0s 882us/step - loss: 0.1958 - recall: 0.9263 - val_loss: 0.1986 - val_recall: 0.9268\n",
            "Epoch 19/300\n",
            "156/156 [==============================] - 0s 958us/step - loss: 0.1932 - recall: 0.9263 - val_loss: 0.1966 - val_recall: 0.9243\n",
            "Epoch 20/300\n",
            "156/156 [==============================] - 0s 929us/step - loss: 0.1908 - recall: 0.9258 - val_loss: 0.1937 - val_recall: 0.9249\n",
            "Epoch 21/300\n",
            "156/156 [==============================] - 0s 942us/step - loss: 0.1889 - recall: 0.9265 - val_loss: 0.1918 - val_recall: 0.9262\n",
            "Epoch 22/300\n",
            "156/156 [==============================] - 0s 913us/step - loss: 0.1867 - recall: 0.9273 - val_loss: 0.1899 - val_recall: 0.9268\n",
            "Epoch 23/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.1846 - recall: 0.9306 - val_loss: 0.1899 - val_recall: 0.9249\n",
            "Epoch 24/300\n",
            "156/156 [==============================] - 0s 902us/step - loss: 0.1824 - recall: 0.9302 - val_loss: 0.1871 - val_recall: 0.9307\n",
            "Epoch 25/300\n",
            "156/156 [==============================] - 0s 891us/step - loss: 0.1807 - recall: 0.9311 - val_loss: 0.1841 - val_recall: 0.9268\n",
            "Epoch 26/300\n",
            "156/156 [==============================] - 0s 911us/step - loss: 0.1788 - recall: 0.9321 - val_loss: 0.1830 - val_recall: 0.9313\n",
            "Epoch 27/300\n",
            "156/156 [==============================] - 0s 901us/step - loss: 0.1773 - recall: 0.9324 - val_loss: 0.1824 - val_recall: 0.9320\n",
            "Epoch 28/300\n",
            "156/156 [==============================] - 0s 888us/step - loss: 0.1759 - recall: 0.9311 - val_loss: 0.1789 - val_recall: 0.9345\n",
            "Epoch 29/300\n",
            "156/156 [==============================] - 0s 898us/step - loss: 0.1744 - recall: 0.9345 - val_loss: 0.1773 - val_recall: 0.9339\n",
            "Epoch 30/300\n",
            "156/156 [==============================] - 0s 911us/step - loss: 0.1728 - recall: 0.9345 - val_loss: 0.1768 - val_recall: 0.9326\n",
            "Epoch 31/300\n",
            "156/156 [==============================] - 0s 890us/step - loss: 0.1710 - recall: 0.9363 - val_loss: 0.1746 - val_recall: 0.9352\n",
            "Epoch 32/300\n",
            "156/156 [==============================] - 0s 908us/step - loss: 0.1693 - recall: 0.9364 - val_loss: 0.1742 - val_recall: 0.9384\n",
            "Epoch 33/300\n",
            "156/156 [==============================] - 0s 891us/step - loss: 0.1680 - recall: 0.9363 - val_loss: 0.1737 - val_recall: 0.9320\n",
            "Epoch 34/300\n",
            "156/156 [==============================] - 0s 920us/step - loss: 0.1670 - recall: 0.9366 - val_loss: 0.1709 - val_recall: 0.9345\n",
            "Epoch 35/300\n",
            "156/156 [==============================] - 0s 773us/step - loss: 0.1651 - recall: 0.9374 - val_loss: 0.1714 - val_recall: 0.9345\n",
            "Epoch 36/300\n",
            "156/156 [==============================] - 0s 913us/step - loss: 0.1638 - recall: 0.9387 - val_loss: 0.1678 - val_recall: 0.9345\n",
            "Epoch 37/300\n",
            "156/156 [==============================] - 0s 924us/step - loss: 0.1627 - recall: 0.9377 - val_loss: 0.1662 - val_recall: 0.9397\n",
            "Epoch 38/300\n",
            "156/156 [==============================] - 0s 943us/step - loss: 0.1616 - recall: 0.9385 - val_loss: 0.1653 - val_recall: 0.9390\n",
            "Epoch 39/300\n",
            "156/156 [==============================] - 0s 878us/step - loss: 0.1600 - recall: 0.9383 - val_loss: 0.1644 - val_recall: 0.9365\n",
            "Epoch 40/300\n",
            "156/156 [==============================] - 0s 908us/step - loss: 0.1583 - recall: 0.9412 - val_loss: 0.1626 - val_recall: 0.9403\n",
            "Epoch 41/300\n",
            "156/156 [==============================] - 0s 946us/step - loss: 0.1574 - recall: 0.9417 - val_loss: 0.1612 - val_recall: 0.9377\n",
            "Epoch 42/300\n",
            "156/156 [==============================] - 0s 899us/step - loss: 0.1563 - recall: 0.9411 - val_loss: 0.1585 - val_recall: 0.9371\n",
            "Epoch 43/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1547 - recall: 0.9403 - val_loss: 0.1611 - val_recall: 0.9403\n",
            "Epoch 44/300\n",
            "156/156 [==============================] - 0s 916us/step - loss: 0.1534 - recall: 0.9424 - val_loss: 0.1583 - val_recall: 0.9345\n",
            "Epoch 45/300\n",
            "156/156 [==============================] - 0s 942us/step - loss: 0.1522 - recall: 0.9403 - val_loss: 0.1552 - val_recall: 0.9416\n",
            "Epoch 46/300\n",
            "156/156 [==============================] - 0s 952us/step - loss: 0.1515 - recall: 0.9427 - val_loss: 0.1539 - val_recall: 0.9397\n",
            "Epoch 47/300\n",
            "156/156 [==============================] - 0s 883us/step - loss: 0.1504 - recall: 0.9433 - val_loss: 0.1531 - val_recall: 0.9403\n",
            "Epoch 48/300\n",
            "156/156 [==============================] - 0s 887us/step - loss: 0.1495 - recall: 0.9432 - val_loss: 0.1509 - val_recall: 0.9403\n",
            "Epoch 49/300\n",
            "156/156 [==============================] - 0s 898us/step - loss: 0.1482 - recall: 0.9448 - val_loss: 0.1504 - val_recall: 0.9409\n",
            "Epoch 50/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1473 - recall: 0.9441 - val_loss: 0.1527 - val_recall: 0.9377\n",
            "Epoch 51/300\n",
            "156/156 [==============================] - 0s 887us/step - loss: 0.1456 - recall: 0.9444 - val_loss: 0.1497 - val_recall: 0.9422\n",
            "Epoch 52/300\n",
            "156/156 [==============================] - 0s 898us/step - loss: 0.1456 - recall: 0.9451 - val_loss: 0.1480 - val_recall: 0.9422\n",
            "Epoch 53/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.1443 - recall: 0.9454 - val_loss: 0.1518 - val_recall: 0.9397\n",
            "Epoch 54/300\n",
            "156/156 [==============================] - 0s 874us/step - loss: 0.1437 - recall: 0.9459 - val_loss: 0.1466 - val_recall: 0.9416\n",
            "Epoch 55/300\n",
            "156/156 [==============================] - 0s 898us/step - loss: 0.1433 - recall: 0.9467 - val_loss: 0.1466 - val_recall: 0.9435\n",
            "Epoch 56/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.1421 - recall: 0.9459 - val_loss: 0.1484 - val_recall: 0.9409\n",
            "Epoch 57/300\n",
            "156/156 [==============================] - 0s 888us/step - loss: 0.1416 - recall: 0.9462 - val_loss: 0.1444 - val_recall: 0.9442\n",
            "Epoch 58/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 0s 908us/step - loss: 0.1409 - recall: 0.9461 - val_loss: 0.1443 - val_recall: 0.9435\n",
            "Epoch 59/300\n",
            "156/156 [==============================] - 0s 799us/step - loss: 0.1391 - recall: 0.9473 - val_loss: 0.1468 - val_recall: 0.9435\n",
            "Epoch 60/300\n",
            "156/156 [==============================] - 0s 870us/step - loss: 0.1390 - recall: 0.9475 - val_loss: 0.1433 - val_recall: 0.9442\n",
            "Epoch 61/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1380 - recall: 0.9467 - val_loss: 0.1436 - val_recall: 0.9461\n",
            "Epoch 62/300\n",
            "156/156 [==============================] - 0s 877us/step - loss: 0.1383 - recall: 0.9486 - val_loss: 0.1419 - val_recall: 0.9461\n",
            "Epoch 63/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1364 - recall: 0.9493 - val_loss: 0.1457 - val_recall: 0.9499\n",
            "Epoch 64/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1369 - recall: 0.9491 - val_loss: 0.1439 - val_recall: 0.9461\n",
            "Epoch 65/300\n",
            "156/156 [==============================] - 0s 887us/step - loss: 0.1362 - recall: 0.9472 - val_loss: 0.1403 - val_recall: 0.9487\n",
            "Epoch 66/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1353 - recall: 0.9499 - val_loss: 0.1422 - val_recall: 0.9467\n",
            "Epoch 67/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1349 - recall: 0.9489 - val_loss: 0.1424 - val_recall: 0.9461\n",
            "Epoch 68/300\n",
            "156/156 [==============================] - 0s 915us/step - loss: 0.1345 - recall: 0.9494 - val_loss: 0.1393 - val_recall: 0.9480\n",
            "Epoch 69/300\n",
            "156/156 [==============================] - 0s 905us/step - loss: 0.1338 - recall: 0.9499 - val_loss: 0.1390 - val_recall: 0.9467\n",
            "Epoch 70/300\n",
            "156/156 [==============================] - 0s 780us/step - loss: 0.1330 - recall: 0.9491 - val_loss: 0.1395 - val_recall: 0.9467\n",
            "Epoch 71/300\n",
            "156/156 [==============================] - 0s 780us/step - loss: 0.1327 - recall: 0.9488 - val_loss: 0.1392 - val_recall: 0.9461\n",
            "Epoch 72/300\n",
            "156/156 [==============================] - 0s 895us/step - loss: 0.1326 - recall: 0.9513 - val_loss: 0.1387 - val_recall: 0.9467\n",
            "Epoch 73/300\n",
            "156/156 [==============================] - 0s 883us/step - loss: 0.1317 - recall: 0.9505 - val_loss: 0.1377 - val_recall: 0.9461\n",
            "Epoch 74/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.1299 - recall: 0.9520 - val_loss: 0.1419 - val_recall: 0.9493\n",
            "Epoch 75/300\n",
            "156/156 [==============================] - 0s 895us/step - loss: 0.1309 - recall: 0.9509 - val_loss: 0.1373 - val_recall: 0.9512\n",
            "Epoch 76/300\n",
            "156/156 [==============================] - 0s 909us/step - loss: 0.1299 - recall: 0.9523 - val_loss: 0.1370 - val_recall: 0.9461\n",
            "Epoch 77/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1299 - recall: 0.9513 - val_loss: 0.1375 - val_recall: 0.9467\n",
            "Epoch 78/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.1284 - recall: 0.9520 - val_loss: 0.1419 - val_recall: 0.9435\n",
            "Epoch 79/300\n",
            "156/156 [==============================] - 0s 760us/step - loss: 0.1287 - recall: 0.9530 - val_loss: 0.1402 - val_recall: 0.9442\n",
            "Epoch 80/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1280 - recall: 0.9526 - val_loss: 0.1376 - val_recall: 0.9474\n",
            "Epoch 81/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1277 - recall: 0.9522 - val_loss: 0.1373 - val_recall: 0.9480\n",
            "Epoch 82/300\n",
            "156/156 [==============================] - 0s 885us/step - loss: 0.1275 - recall: 0.9515 - val_loss: 0.1364 - val_recall: 0.9480\n",
            "Epoch 83/300\n",
            "156/156 [==============================] - 0s 781us/step - loss: 0.1266 - recall: 0.9526 - val_loss: 0.1380 - val_recall: 0.9461\n",
            "Epoch 84/300\n",
            "156/156 [==============================] - 0s 896us/step - loss: 0.1263 - recall: 0.9530 - val_loss: 0.1363 - val_recall: 0.9519\n",
            "Epoch 85/300\n",
            "156/156 [==============================] - 0s 895us/step - loss: 0.1261 - recall: 0.9538 - val_loss: 0.1358 - val_recall: 0.9454\n",
            "Epoch 86/300\n",
            "156/156 [==============================] - 0s 890us/step - loss: 0.1260 - recall: 0.9513 - val_loss: 0.1353 - val_recall: 0.9512\n",
            "Epoch 87/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.1253 - recall: 0.9523 - val_loss: 0.1355 - val_recall: 0.9487\n",
            "Epoch 88/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1252 - recall: 0.9538 - val_loss: 0.1362 - val_recall: 0.9487\n",
            "Epoch 89/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1241 - recall: 0.9528 - val_loss: 0.1369 - val_recall: 0.9512\n",
            "Epoch 90/300\n",
            "156/156 [==============================] - 0s 898us/step - loss: 0.1244 - recall: 0.9531 - val_loss: 0.1343 - val_recall: 0.9493\n",
            "Epoch 91/300\n",
            "156/156 [==============================] - 0s 885us/step - loss: 0.1235 - recall: 0.9542 - val_loss: 0.1340 - val_recall: 0.9487\n",
            "Epoch 92/300\n",
            "156/156 [==============================] - 0s 905us/step - loss: 0.1240 - recall: 0.9528 - val_loss: 0.1334 - val_recall: 0.9506\n",
            "Epoch 93/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1233 - recall: 0.9552 - val_loss: 0.1336 - val_recall: 0.9512\n",
            "Epoch 94/300\n",
            "156/156 [==============================] - 0s 891us/step - loss: 0.1227 - recall: 0.9542 - val_loss: 0.1328 - val_recall: 0.9512\n",
            "Epoch 95/300\n",
            "156/156 [==============================] - 0s 881us/step - loss: 0.1224 - recall: 0.9550 - val_loss: 0.1322 - val_recall: 0.9519\n",
            "Epoch 96/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1217 - recall: 0.9544 - val_loss: 0.1337 - val_recall: 0.9525\n",
            "Epoch 97/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1214 - recall: 0.9557 - val_loss: 0.1322 - val_recall: 0.9493\n",
            "Epoch 98/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1206 - recall: 0.9552 - val_loss: 0.1323 - val_recall: 0.9525\n",
            "Epoch 99/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1205 - recall: 0.9549 - val_loss: 0.1335 - val_recall: 0.9493\n",
            "Epoch 100/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1198 - recall: 0.9550 - val_loss: 0.1332 - val_recall: 0.9525\n",
            "Epoch 101/300\n",
            "156/156 [==============================] - 0s 895us/step - loss: 0.1198 - recall: 0.9546 - val_loss: 0.1320 - val_recall: 0.9506\n",
            "Epoch 102/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.1191 - recall: 0.9550 - val_loss: 0.1336 - val_recall: 0.9499\n",
            "Epoch 103/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.1194 - recall: 0.9550 - val_loss: 0.1328 - val_recall: 0.9506\n",
            "Epoch 104/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1187 - recall: 0.9557 - val_loss: 0.1344 - val_recall: 0.9519\n",
            "Epoch 105/300\n",
            "156/156 [==============================] - 0s 902us/step - loss: 0.1188 - recall: 0.9562 - val_loss: 0.1314 - val_recall: 0.9531\n",
            "Epoch 106/300\n",
            "156/156 [==============================] - 0s 780us/step - loss: 0.1181 - recall: 0.9550 - val_loss: 0.1322 - val_recall: 0.9519\n",
            "Epoch 107/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1173 - recall: 0.9575 - val_loss: 0.1318 - val_recall: 0.9531\n",
            "Epoch 108/300\n",
            "156/156 [==============================] - 0s 729us/step - loss: 0.1172 - recall: 0.9552 - val_loss: 0.1319 - val_recall: 0.9506\n",
            "Epoch 109/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1174 - recall: 0.9575 - val_loss: 0.1317 - val_recall: 0.9551\n",
            "Epoch 110/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1172 - recall: 0.9550 - val_loss: 0.1316 - val_recall: 0.9544\n",
            "Epoch 111/300\n",
            "156/156 [==============================] - 0s 875us/step - loss: 0.1171 - recall: 0.9565 - val_loss: 0.1312 - val_recall: 0.9531\n",
            "Epoch 112/300\n",
            "156/156 [==============================] - 0s 897us/step - loss: 0.1165 - recall: 0.9566 - val_loss: 0.1307 - val_recall: 0.9512\n",
            "Epoch 113/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.1159 - recall: 0.9563 - val_loss: 0.1317 - val_recall: 0.9544\n",
            "Epoch 114/300\n",
            "156/156 [==============================] - 0s 891us/step - loss: 0.1156 - recall: 0.9566 - val_loss: 0.1300 - val_recall: 0.9531\n",
            "Epoch 115/300\n",
            "156/156 [==============================] - 0s 882us/step - loss: 0.1153 - recall: 0.9565 - val_loss: 0.1296 - val_recall: 0.9525\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 116/300\n",
            "156/156 [==============================] - 0s 780us/step - loss: 0.1149 - recall: 0.9573 - val_loss: 0.1303 - val_recall: 0.9538\n",
            "Epoch 117/300\n",
            "156/156 [==============================] - 0s 912us/step - loss: 0.1150 - recall: 0.9566 - val_loss: 0.1291 - val_recall: 0.9538\n",
            "Epoch 118/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1143 - recall: 0.9571 - val_loss: 0.1316 - val_recall: 0.9525\n",
            "Epoch 119/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1141 - recall: 0.9581 - val_loss: 0.1316 - val_recall: 0.9512\n",
            "Epoch 120/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1137 - recall: 0.9575 - val_loss: 0.1312 - val_recall: 0.9512\n",
            "Epoch 121/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1135 - recall: 0.9586 - val_loss: 0.1335 - val_recall: 0.9480\n",
            "Epoch 122/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1126 - recall: 0.9584 - val_loss: 0.1292 - val_recall: 0.9538\n",
            "Epoch 123/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1120 - recall: 0.9579 - val_loss: 0.1314 - val_recall: 0.9512\n",
            "Epoch 124/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.1129 - recall: 0.9571 - val_loss: 0.1292 - val_recall: 0.9512\n",
            "Epoch 125/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1121 - recall: 0.9579 - val_loss: 0.1302 - val_recall: 0.9531\n",
            "Epoch 126/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.1116 - recall: 0.9573 - val_loss: 0.1293 - val_recall: 0.9544\n",
            "Epoch 127/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1116 - recall: 0.9592 - val_loss: 0.1343 - val_recall: 0.9467\n",
            "Epoch 128/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1109 - recall: 0.9581 - val_loss: 0.1294 - val_recall: 0.9538\n",
            "Epoch 129/300\n",
            "156/156 [==============================] - 0s 876us/step - loss: 0.1110 - recall: 0.9583 - val_loss: 0.1283 - val_recall: 0.9525\n",
            "Epoch 130/300\n",
            "156/156 [==============================] - 0s 758us/step - loss: 0.1099 - recall: 0.9595 - val_loss: 0.1298 - val_recall: 0.9531\n",
            "Epoch 131/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1094 - recall: 0.9591 - val_loss: 0.1331 - val_recall: 0.9487\n",
            "Epoch 132/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1101 - recall: 0.9597 - val_loss: 0.1289 - val_recall: 0.9525\n",
            "Epoch 133/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1095 - recall: 0.9605 - val_loss: 0.1284 - val_recall: 0.9531\n",
            "Epoch 134/300\n",
            "156/156 [==============================] - 0s 758us/step - loss: 0.1090 - recall: 0.9599 - val_loss: 0.1284 - val_recall: 0.9531\n",
            "Epoch 135/300\n",
            "156/156 [==============================] - 0s 876us/step - loss: 0.1086 - recall: 0.9591 - val_loss: 0.1279 - val_recall: 0.9544\n",
            "Epoch 136/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1085 - recall: 0.9595 - val_loss: 0.1300 - val_recall: 0.9538\n",
            "Epoch 137/300\n",
            "156/156 [==============================] - 0s 780us/step - loss: 0.1086 - recall: 0.9602 - val_loss: 0.1293 - val_recall: 0.9493\n",
            "Epoch 138/300\n",
            "156/156 [==============================] - 0s 780us/step - loss: 0.1079 - recall: 0.9605 - val_loss: 0.1280 - val_recall: 0.9531\n",
            "Epoch 139/300\n",
            "156/156 [==============================] - 0s 900us/step - loss: 0.1080 - recall: 0.9603 - val_loss: 0.1278 - val_recall: 0.9531\n",
            "Epoch 140/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1076 - recall: 0.9597 - val_loss: 0.1295 - val_recall: 0.9538\n",
            "Epoch 141/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1075 - recall: 0.9603 - val_loss: 0.1279 - val_recall: 0.9525\n",
            "Epoch 142/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.1070 - recall: 0.9603 - val_loss: 0.1282 - val_recall: 0.9538\n",
            "Epoch 143/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1056 - recall: 0.9613 - val_loss: 0.1288 - val_recall: 0.9538\n",
            "Epoch 144/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1061 - recall: 0.9618 - val_loss: 0.1313 - val_recall: 0.9538\n",
            "Epoch 145/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1061 - recall: 0.9607 - val_loss: 0.1287 - val_recall: 0.9544\n",
            "Epoch 146/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1058 - recall: 0.9621 - val_loss: 0.1292 - val_recall: 0.9531\n",
            "Epoch 147/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1059 - recall: 0.9619 - val_loss: 0.1288 - val_recall: 0.9525\n",
            "Epoch 148/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.1055 - recall: 0.9623 - val_loss: 0.1285 - val_recall: 0.9531\n",
            "Epoch 149/300\n",
            "156/156 [==============================] - 0s 912us/step - loss: 0.1044 - recall: 0.9626 - val_loss: 0.1277 - val_recall: 0.9544\n",
            "Epoch 150/300\n",
            "156/156 [==============================] - 0s 927us/step - loss: 0.1048 - recall: 0.9616 - val_loss: 0.1277 - val_recall: 0.9544\n",
            "Epoch 151/300\n",
            "156/156 [==============================] - 0s 888us/step - loss: 0.1046 - recall: 0.9613 - val_loss: 0.1273 - val_recall: 0.9551\n",
            "Epoch 152/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1042 - recall: 0.9629 - val_loss: 0.1293 - val_recall: 0.9538\n",
            "Epoch 153/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1036 - recall: 0.9616 - val_loss: 0.1322 - val_recall: 0.9551\n",
            "Epoch 154/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1030 - recall: 0.9627 - val_loss: 0.1319 - val_recall: 0.9538\n",
            "Epoch 155/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1031 - recall: 0.9636 - val_loss: 0.1277 - val_recall: 0.9538\n",
            "Epoch 156/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1027 - recall: 0.9632 - val_loss: 0.1305 - val_recall: 0.9557\n",
            "Epoch 157/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1033 - recall: 0.9624 - val_loss: 0.1275 - val_recall: 0.9538\n",
            "Epoch 158/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1030 - recall: 0.9640 - val_loss: 0.1292 - val_recall: 0.9538\n",
            "Epoch 159/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1022 - recall: 0.9634 - val_loss: 0.1290 - val_recall: 0.9557\n",
            "Epoch 160/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.1029 - recall: 0.9634 - val_loss: 0.1292 - val_recall: 0.9551\n",
            "Epoch 161/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.1020 - recall: 0.9634 - val_loss: 0.1296 - val_recall: 0.9544\n",
            "Epoch 162/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1019 - recall: 0.9634 - val_loss: 0.1289 - val_recall: 0.9538\n",
            "Epoch 163/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1018 - recall: 0.9626 - val_loss: 0.1275 - val_recall: 0.9557\n",
            "Epoch 164/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.1016 - recall: 0.9627 - val_loss: 0.1277 - val_recall: 0.9538\n",
            "Epoch 165/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.1016 - recall: 0.9624 - val_loss: 0.1282 - val_recall: 0.9544\n",
            "Epoch 166/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0998 - recall: 0.9636 - val_loss: 0.1287 - val_recall: 0.9512\n",
            "Epoch 167/300\n",
            "156/156 [==============================] - 0s 764us/step - loss: 0.1002 - recall: 0.9626 - val_loss: 0.1287 - val_recall: 0.9544\n",
            "Epoch 168/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.1004 - recall: 0.9629 - val_loss: 0.1282 - val_recall: 0.9531\n",
            "Epoch 169/300\n",
            "156/156 [==============================] - 0s 764us/step - loss: 0.1001 - recall: 0.9645 - val_loss: 0.1292 - val_recall: 0.9519\n",
            "Epoch 170/300\n",
            "156/156 [==============================] - 0s 785us/step - loss: 0.1000 - recall: 0.9645 - val_loss: 0.1279 - val_recall: 0.9538\n",
            "Epoch 171/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0994 - recall: 0.9652 - val_loss: 0.1290 - val_recall: 0.9544\n",
            "Epoch 172/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0993 - recall: 0.9642 - val_loss: 0.1274 - val_recall: 0.9544\n",
            "Epoch 173/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 0s 748us/step - loss: 0.0988 - recall: 0.9639 - val_loss: 0.1336 - val_recall: 0.9493\n",
            "Epoch 174/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0986 - recall: 0.9647 - val_loss: 0.1290 - val_recall: 0.9544\n",
            "Epoch 175/300\n",
            "156/156 [==============================] - 0s 786us/step - loss: 0.0988 - recall: 0.9658 - val_loss: 0.1279 - val_recall: 0.9519\n",
            "Epoch 176/300\n",
            "156/156 [==============================] - 0s 900us/step - loss: 0.0982 - recall: 0.9647 - val_loss: 0.1266 - val_recall: 0.9544\n",
            "Epoch 177/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0980 - recall: 0.9652 - val_loss: 0.1281 - val_recall: 0.9538\n",
            "Epoch 178/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0976 - recall: 0.9650 - val_loss: 0.1278 - val_recall: 0.9551\n",
            "Epoch 179/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0970 - recall: 0.9652 - val_loss: 0.1281 - val_recall: 0.9531\n",
            "Epoch 180/300\n",
            "156/156 [==============================] - 0s 758us/step - loss: 0.0971 - recall: 0.9660 - val_loss: 0.1311 - val_recall: 0.9531\n",
            "Epoch 181/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0974 - recall: 0.9645 - val_loss: 0.1279 - val_recall: 0.9544\n",
            "Epoch 182/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0962 - recall: 0.9664 - val_loss: 0.1298 - val_recall: 0.9551\n",
            "Epoch 183/300\n",
            "156/156 [==============================] - 0s 816us/step - loss: 0.0970 - recall: 0.9647 - val_loss: 0.1286 - val_recall: 0.9531\n",
            "Epoch 184/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0962 - recall: 0.9652 - val_loss: 0.1313 - val_recall: 0.9557\n",
            "Epoch 185/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.0958 - recall: 0.9650 - val_loss: 0.1305 - val_recall: 0.9538\n",
            "Epoch 186/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0960 - recall: 0.9648 - val_loss: 0.1279 - val_recall: 0.9551\n",
            "Epoch 187/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0957 - recall: 0.9647 - val_loss: 0.1273 - val_recall: 0.9551\n",
            "Epoch 188/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0956 - recall: 0.9652 - val_loss: 0.1272 - val_recall: 0.9551\n",
            "Epoch 189/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0953 - recall: 0.9663 - val_loss: 0.1276 - val_recall: 0.9544\n",
            "Epoch 190/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0952 - recall: 0.9658 - val_loss: 0.1282 - val_recall: 0.9525\n",
            "Epoch 191/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0942 - recall: 0.9652 - val_loss: 0.1269 - val_recall: 0.9531\n",
            "Epoch 192/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0944 - recall: 0.9669 - val_loss: 0.1285 - val_recall: 0.9544\n",
            "Epoch 193/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0939 - recall: 0.9663 - val_loss: 0.1279 - val_recall: 0.9544\n",
            "Epoch 194/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0944 - recall: 0.9661 - val_loss: 0.1266 - val_recall: 0.9538\n",
            "Epoch 195/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0936 - recall: 0.9653 - val_loss: 0.1316 - val_recall: 0.9519\n",
            "Epoch 196/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0933 - recall: 0.9658 - val_loss: 0.1275 - val_recall: 0.9544\n",
            "Epoch 197/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0937 - recall: 0.9672 - val_loss: 0.1276 - val_recall: 0.9570\n",
            "Epoch 198/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0929 - recall: 0.9666 - val_loss: 0.1286 - val_recall: 0.9551\n",
            "Epoch 199/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0930 - recall: 0.9669 - val_loss: 0.1289 - val_recall: 0.9557\n",
            "Epoch 200/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0926 - recall: 0.9661 - val_loss: 0.1301 - val_recall: 0.9557\n",
            "Epoch 201/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0929 - recall: 0.9664 - val_loss: 0.1279 - val_recall: 0.9551\n",
            "Epoch 202/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0920 - recall: 0.9671 - val_loss: 0.1277 - val_recall: 0.9538\n",
            "Epoch 203/300\n",
            "156/156 [==============================] - 0s 759us/step - loss: 0.0914 - recall: 0.9677 - val_loss: 0.1282 - val_recall: 0.9525\n",
            "Epoch 204/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0917 - recall: 0.9666 - val_loss: 0.1273 - val_recall: 0.9551\n",
            "Epoch 205/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0907 - recall: 0.9687 - val_loss: 0.1282 - val_recall: 0.9531\n",
            "Epoch 206/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0907 - recall: 0.9671 - val_loss: 0.1295 - val_recall: 0.9557\n",
            "Epoch 207/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.0913 - recall: 0.9671 - val_loss: 0.1325 - val_recall: 0.9538\n",
            "Epoch 208/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0911 - recall: 0.9668 - val_loss: 0.1284 - val_recall: 0.9544\n",
            "Epoch 209/300\n",
            "156/156 [==============================] - 0s 807us/step - loss: 0.0909 - recall: 0.9679 - val_loss: 0.1292 - val_recall: 0.9538\n",
            "Epoch 210/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.0898 - recall: 0.9701 - val_loss: 0.1327 - val_recall: 0.9564\n",
            "Epoch 211/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0897 - recall: 0.9687 - val_loss: 0.1329 - val_recall: 0.9557\n",
            "Epoch 212/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0889 - recall: 0.9690 - val_loss: 0.1333 - val_recall: 0.9519\n",
            "Epoch 213/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.0899 - recall: 0.9672 - val_loss: 0.1306 - val_recall: 0.9557\n",
            "Epoch 214/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0892 - recall: 0.9674 - val_loss: 0.1283 - val_recall: 0.9525\n",
            "Epoch 215/300\n",
            "156/156 [==============================] - 0s 729us/step - loss: 0.0894 - recall: 0.9666 - val_loss: 0.1270 - val_recall: 0.9525\n",
            "Epoch 216/300\n",
            "156/156 [==============================] - 0s 753us/step - loss: 0.0893 - recall: 0.9689 - val_loss: 0.1299 - val_recall: 0.9538\n",
            "Epoch 217/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.0887 - recall: 0.9679 - val_loss: 0.1286 - val_recall: 0.9557\n",
            "Epoch 218/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0894 - recall: 0.9672 - val_loss: 0.1291 - val_recall: 0.9557\n",
            "Epoch 219/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0884 - recall: 0.9679 - val_loss: 0.1327 - val_recall: 0.9531\n",
            "Epoch 220/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.0888 - recall: 0.9692 - val_loss: 0.1282 - val_recall: 0.9538\n",
            "Epoch 221/300\n",
            "156/156 [==============================] - 0s 778us/step - loss: 0.0881 - recall: 0.9682 - val_loss: 0.1297 - val_recall: 0.9525\n",
            "Epoch 222/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0875 - recall: 0.9685 - val_loss: 0.1294 - val_recall: 0.9557\n",
            "Epoch 223/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0879 - recall: 0.9684 - val_loss: 0.1289 - val_recall: 0.9519\n",
            "Epoch 224/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0870 - recall: 0.9692 - val_loss: 0.1283 - val_recall: 0.9531\n",
            "Epoch 225/300\n",
            "156/156 [==============================] - 0s 806us/step - loss: 0.0871 - recall: 0.9693 - val_loss: 0.1295 - val_recall: 0.9531\n",
            "Epoch 226/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0873 - recall: 0.9682 - val_loss: 0.1331 - val_recall: 0.9499\n",
            "Epoch 227/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0867 - recall: 0.9689 - val_loss: 0.1297 - val_recall: 0.9544\n",
            "Epoch 228/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0859 - recall: 0.9685 - val_loss: 0.1341 - val_recall: 0.9525\n",
            "Epoch 229/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.0868 - recall: 0.9687 - val_loss: 0.1293 - val_recall: 0.9544\n",
            "Epoch 230/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 0s 806us/step - loss: 0.0860 - recall: 0.9690 - val_loss: 0.1307 - val_recall: 0.9576\n",
            "Epoch 231/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0857 - recall: 0.9693 - val_loss: 0.1314 - val_recall: 0.9531\n",
            "Epoch 232/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0857 - recall: 0.9682 - val_loss: 0.1318 - val_recall: 0.9551\n",
            "Epoch 233/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0855 - recall: 0.9695 - val_loss: 0.1292 - val_recall: 0.9576\n",
            "Epoch 234/300\n",
            "156/156 [==============================] - 0s 818us/step - loss: 0.0847 - recall: 0.9706 - val_loss: 0.1303 - val_recall: 0.9551\n",
            "Epoch 235/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0850 - recall: 0.9697 - val_loss: 0.1300 - val_recall: 0.9551\n",
            "Epoch 236/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0841 - recall: 0.9706 - val_loss: 0.1303 - val_recall: 0.9538\n",
            "Epoch 237/300\n",
            "156/156 [==============================] - 0s 780us/step - loss: 0.0842 - recall: 0.9708 - val_loss: 0.1320 - val_recall: 0.9557\n",
            "Epoch 238/300\n",
            "156/156 [==============================] - 0s 803us/step - loss: 0.0840 - recall: 0.9709 - val_loss: 0.1336 - val_recall: 0.9519\n",
            "Epoch 239/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0846 - recall: 0.9705 - val_loss: 0.1324 - val_recall: 0.9557\n",
            "Epoch 240/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0842 - recall: 0.9695 - val_loss: 0.1329 - val_recall: 0.9538\n",
            "Epoch 241/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0839 - recall: 0.9708 - val_loss: 0.1303 - val_recall: 0.9564\n",
            "Epoch 242/300\n",
            "156/156 [==============================] - 0s 780us/step - loss: 0.0827 - recall: 0.9706 - val_loss: 0.1325 - val_recall: 0.9570\n",
            "Epoch 243/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.0832 - recall: 0.9717 - val_loss: 0.1306 - val_recall: 0.9538\n",
            "Epoch 244/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0823 - recall: 0.9721 - val_loss: 0.1322 - val_recall: 0.9551\n",
            "Epoch 245/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0827 - recall: 0.9706 - val_loss: 0.1311 - val_recall: 0.9538\n",
            "Epoch 246/300\n",
            "156/156 [==============================] - 0s 959us/step - loss: 0.0821 - recall: 0.9721 - val_loss: 0.1315 - val_recall: 0.9525\n",
            "Epoch 247/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0818 - recall: 0.9714 - val_loss: 0.1337 - val_recall: 0.9538\n",
            "Epoch 248/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0831 - recall: 0.9701 - val_loss: 0.1291 - val_recall: 0.9551\n",
            "Epoch 249/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0810 - recall: 0.9716 - val_loss: 0.1299 - val_recall: 0.9551\n",
            "Epoch 250/300\n",
            "156/156 [==============================] - 0s 758us/step - loss: 0.0812 - recall: 0.9719 - val_loss: 0.1352 - val_recall: 0.9544\n",
            "Epoch 251/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0804 - recall: 0.9719 - val_loss: 0.1304 - val_recall: 0.9551\n",
            "Epoch 252/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0808 - recall: 0.9716 - val_loss: 0.1323 - val_recall: 0.9557\n",
            "Epoch 253/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0812 - recall: 0.9724 - val_loss: 0.1297 - val_recall: 0.9564\n",
            "Epoch 254/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0796 - recall: 0.9724 - val_loss: 0.1296 - val_recall: 0.9564\n",
            "Epoch 255/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.0800 - recall: 0.9717 - val_loss: 0.1343 - val_recall: 0.9564\n",
            "Epoch 256/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.0808 - recall: 0.9713 - val_loss: 0.1316 - val_recall: 0.9525\n",
            "Epoch 257/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0802 - recall: 0.9724 - val_loss: 0.1312 - val_recall: 0.9564\n",
            "Epoch 258/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0792 - recall: 0.9714 - val_loss: 0.1365 - val_recall: 0.9557\n",
            "Epoch 259/300\n",
            "156/156 [==============================] - 0s 749us/step - loss: 0.0804 - recall: 0.9722 - val_loss: 0.1336 - val_recall: 0.9519\n",
            "Epoch 260/300\n",
            "156/156 [==============================] - 0s 746us/step - loss: 0.0780 - recall: 0.9722 - val_loss: 0.1389 - val_recall: 0.9576\n",
            "Epoch 261/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0783 - recall: 0.9741 - val_loss: 0.1335 - val_recall: 0.9576\n",
            "Epoch 262/300\n",
            "156/156 [==============================] - 0s 847us/step - loss: 0.0794 - recall: 0.9733 - val_loss: 0.1316 - val_recall: 0.9570\n",
            "Epoch 263/300\n",
            "156/156 [==============================] - 0s 774us/step - loss: 0.0782 - recall: 0.9740 - val_loss: 0.1316 - val_recall: 0.9576\n",
            "Epoch 264/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0773 - recall: 0.9735 - val_loss: 0.1308 - val_recall: 0.9564\n",
            "Epoch 265/300\n",
            "156/156 [==============================] - 0s 738us/step - loss: 0.0782 - recall: 0.9725 - val_loss: 0.1318 - val_recall: 0.9564\n",
            "Epoch 266/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0772 - recall: 0.9729 - val_loss: 0.1360 - val_recall: 0.9512\n",
            "Epoch 267/300\n",
            "156/156 [==============================] - 0s 769us/step - loss: 0.0781 - recall: 0.9724 - val_loss: 0.1306 - val_recall: 0.9551\n",
            "Epoch 268/300\n",
            "156/156 [==============================] - 0s 729us/step - loss: 0.0777 - recall: 0.9735 - val_loss: 0.1318 - val_recall: 0.9551\n",
            "Epoch 269/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0772 - recall: 0.9740 - val_loss: 0.1373 - val_recall: 0.9519\n",
            "Epoch 270/300\n",
            "156/156 [==============================] - 0s 767us/step - loss: 0.0783 - recall: 0.9740 - val_loss: 0.1342 - val_recall: 0.9551\n",
            "Epoch 271/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0773 - recall: 0.9743 - val_loss: 0.1313 - val_recall: 0.9557\n",
            "Epoch 272/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0772 - recall: 0.9741 - val_loss: 0.1316 - val_recall: 0.9564\n",
            "Epoch 273/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0770 - recall: 0.9737 - val_loss: 0.1319 - val_recall: 0.9570\n",
            "Epoch 274/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0762 - recall: 0.9748 - val_loss: 0.1333 - val_recall: 0.9576\n",
            "Epoch 275/300\n",
            "156/156 [==============================] - 0s 749us/step - loss: 0.0762 - recall: 0.9738 - val_loss: 0.1349 - val_recall: 0.9570\n",
            "Epoch 276/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0759 - recall: 0.9746 - val_loss: 0.1375 - val_recall: 0.9544\n",
            "Epoch 277/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0761 - recall: 0.9738 - val_loss: 0.1338 - val_recall: 0.9583\n",
            "Epoch 278/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0751 - recall: 0.9745 - val_loss: 0.1385 - val_recall: 0.9544\n",
            "Epoch 279/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0752 - recall: 0.9754 - val_loss: 0.1381 - val_recall: 0.9531\n",
            "Epoch 280/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0753 - recall: 0.9759 - val_loss: 0.1335 - val_recall: 0.9570\n",
            "Epoch 281/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0758 - recall: 0.9758 - val_loss: 0.1322 - val_recall: 0.9564\n",
            "Epoch 282/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0751 - recall: 0.9751 - val_loss: 0.1348 - val_recall: 0.9596\n",
            "Epoch 283/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0748 - recall: 0.9751 - val_loss: 0.1322 - val_recall: 0.9551\n",
            "Epoch 284/300\n",
            "156/156 [==============================] - 0s 761us/step - loss: 0.0747 - recall: 0.9748 - val_loss: 0.1317 - val_recall: 0.9564\n",
            "Epoch 285/300\n",
            "156/156 [==============================] - 0s 754us/step - loss: 0.0745 - recall: 0.9748 - val_loss: 0.1338 - val_recall: 0.9576\n",
            "Epoch 286/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0748 - recall: 0.9756 - val_loss: 0.1366 - val_recall: 0.9583\n",
            "Epoch 287/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 0s 748us/step - loss: 0.0745 - recall: 0.9759 - val_loss: 0.1328 - val_recall: 0.9564\n",
            "Epoch 288/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0738 - recall: 0.9764 - val_loss: 0.1366 - val_recall: 0.9570\n",
            "Epoch 289/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0738 - recall: 0.9761 - val_loss: 0.1324 - val_recall: 0.9570\n",
            "Epoch 290/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0739 - recall: 0.9751 - val_loss: 0.1326 - val_recall: 0.9564\n",
            "Epoch 291/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0730 - recall: 0.9769 - val_loss: 0.1336 - val_recall: 0.9544\n",
            "Epoch 292/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0730 - recall: 0.9764 - val_loss: 0.1379 - val_recall: 0.9596\n",
            "Epoch 293/300\n",
            "156/156 [==============================] - 0s 748us/step - loss: 0.0726 - recall: 0.9764 - val_loss: 0.1353 - val_recall: 0.9576\n",
            "Epoch 294/300\n",
            "156/156 [==============================] - 0s 729us/step - loss: 0.0728 - recall: 0.9743 - val_loss: 0.1334 - val_recall: 0.9576\n",
            "Epoch 295/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0728 - recall: 0.9762 - val_loss: 0.1339 - val_recall: 0.9589\n",
            "Epoch 296/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0719 - recall: 0.9751 - val_loss: 0.1350 - val_recall: 0.9551\n",
            "Epoch 297/300\n",
            "156/156 [==============================] - 0s 742us/step - loss: 0.0721 - recall: 0.9770 - val_loss: 0.1336 - val_recall: 0.9576\n",
            "Epoch 298/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0718 - recall: 0.9774 - val_loss: 0.1327 - val_recall: 0.9551\n",
            "Epoch 299/300\n",
            "156/156 [==============================] - 0s 735us/step - loss: 0.0720 - recall: 0.9767 - val_loss: 0.1326 - val_recall: 0.9576\n",
            "Epoch 300/300\n",
            "156/156 [==============================] - 0s 741us/step - loss: 0.0710 - recall: 0.9756 - val_loss: 0.1331 - val_recall: 0.9557\n"
          ]
        }
      ],
      "source": [
        "# Training the network (batch_size= 40, epoch = 300)\n",
        "print('\\nFitting DNN (Detection Module):\\n')\n",
        "batch_size1 = 40\n",
        "history_detection = model_detection.fit(X_trn_detection, t_trn_detection, epochs=300,\n",
        "                                validation_split=.2, batch_size = batch_size1,\n",
        "                                callbacks=callbacks_list, verbose=1)"
      ],
      "id": "rlqhyIv9d0L8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ8o5CxDd0L8",
        "outputId": "c838a568-c1ef-43dd-f50b-1cbfff37dd3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x1cb741ee820>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting the variation of loss function on both training and validation set: \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "val_loss = history_detection.history[\"val_loss\"]\n",
        "loss = history_detection.history[\"loss\"]\n",
        "\n",
        "epochs = range(1, 301)\n",
        "plt.plot(epochs, val_loss[:], \"r-\",\n",
        "label=\"Validation Loss\")\n",
        "plt.plot(epochs, loss[:], \"b-\",\n",
        "label=\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "UJ8o5CxDd0L8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miwQwAdid0L9"
      },
      "outputs": [],
      "source": [
        "# Testin the trained network over the testset.\n",
        "predictions_label = model_detection.predict(X_tst_detection)\n",
        "\n",
        "# Using argmax function to select the label with the highest probability.\n",
        "y_pred = np.zeros([len(X_tst_detection),1])\n",
        "for i in range(len(X_tst_detection)):\n",
        "    y_pred[i,0] = np.argmax(predictions_label[i])"
      ],
      "id": "miwQwAdid0L9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQvVlSHtd0L9"
      },
      "source": [
        "### Metrics"
      ],
      "id": "fQvVlSHtd0L9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaj_FClId0L9",
        "outputId": "5735c691-6ebe-4d28-c340-9c8294c2a6d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " For Rainfall Detection Over Coast:\n",
            "\n",
            "Precision:  0.9443757725587144\n",
            "Accuracy:  0.9730861244019139\n",
            "Recall (TPR):  0.9443757725587144\n",
            "False Alarm (FPR):  0.01775147928994083\n",
            "\n",
            "F1 Score:  0.9443757725587144\n",
            "----------------------------\n",
            "\n",
            " For Snwofall Detection Over Coast:\n",
            "\n",
            "Precision:  0.9551208285385501\n",
            "Accuracy:  0.9823564593301436\n",
            "Recall (TPR):  0.9764705882352941\n",
            "False Alarm (FPR):  0.015637530072173216\n",
            "\n",
            "F1 Score:  0.9656777196044211\n",
            "----------------------------\n",
            "\n",
            " For Model Detection Over Coast:\n",
            "\n",
            "Precision:  0.9563397129186603\n",
            "Recall (TPR):  0.9563397129186603\n",
            "\n",
            "F1 Score:  0.9563397129186603\n"
          ]
        }
      ],
      "source": [
        "### Detection Module\n",
        "\n",
        "n_cc=1\n",
        "n_cs=1\n",
        "n_cr=1\n",
        "n_sc=1\n",
        "n_ss=1\n",
        "n_sr=1\n",
        "n_rc=1\n",
        "n_rs=1\n",
        "n_rr=1\n",
        "\n",
        "precip_c= 0\n",
        "precip_r= 1\n",
        "precip_s= 2\n",
        "\n",
        "for i in range(len(X_tst_detection)):\n",
        "    label_predict = y_pred[i]\n",
        "    label_actual = t_test[i]\n",
        "    if label_predict==precip_c and label_actual==precip_c:\n",
        "        n_cc+=1\n",
        "    if label_predict==precip_s and label_actual==precip_s:\n",
        "        n_ss+=1\n",
        "    if label_predict==precip_r and label_actual==precip_r:\n",
        "        n_rr+=1 \n",
        "    if label_predict==precip_c and label_actual==precip_s:\n",
        "        n_cs+=1\n",
        "    if label_predict==precip_c and label_actual==precip_r:\n",
        "        n_cr+=1\n",
        "    if label_predict==precip_s and label_actual==precip_c:\n",
        "        n_sc+=1\n",
        "    if label_predict==precip_s and label_actual==precip_r:\n",
        "        n_sr+=1  \n",
        "    if label_predict==precip_r and label_actual==precip_c:\n",
        "        n_rc+=1\n",
        "    if label_predict==precip_r and label_actual==precip_s:\n",
        "        n_rs+=1        \n",
        "        \n",
        "#Snow\n",
        "TP_s = n_ss\n",
        "TN_s = n_cc+n_cr+n_rc+n_rr\n",
        "FP_s = n_sc+n_sr\n",
        "FN_s = n_cs+n_rs\n",
        "\n",
        "precision_s = TP_s/(TP_s+FP_s)\n",
        "acc_s = (TP_s+TN_s)/(TP_s+TN_s+FP_s+FN_s)\n",
        "recall_s = TP_s/(TP_s+FN_s)\n",
        "f1_score_s = (2*precision_s*recall_s)/(precision_s+recall_s)\n",
        "FPR_s = FP_s/(FP_s+TN_s)\n",
        "\n",
        "#Rain\n",
        "TP_r = n_rr\n",
        "TN_r = n_cc+n_cs+n_sc+n_ss\n",
        "FP_r = n_rc+n_rs\n",
        "FN_r = n_cr+n_sr\n",
        "\n",
        "precision_r = TP_r/(TP_r+FP_r)\n",
        "acc_r = (TP_r+TN_r)/(TP_r+TN_r+FP_r+FN_r)\n",
        "recall_r = TP_r/(TP_r+FN_r)\n",
        "f1_score_r = (2*precision_r*recall_r)/(precision_r+recall_r)\n",
        "FPR_r = FP_r/(FP_r+TN_r) \n",
        "\n",
        "print('\\n For Rainfall Detection Over Coast:\\n')\n",
        "print('Precision: ',precision_r)\n",
        "print('Accuracy: ',acc_r)\n",
        "print('Recall (TPR): ',recall_r)\n",
        "print('False Alarm (FPR): ',FPR_r)\n",
        "print('\\nF1 Score: ',f1_score_r)\n",
        "\n",
        "print('----------------------------')\n",
        "print('\\n For Snwofall Detection Over Coast:\\n')\n",
        "print('Precision: ',precision_s)\n",
        "print('Accuracy: ',acc_s)\n",
        "print('Recall (TPR): ',recall_s)\n",
        "print('False Alarm (FPR): ',FPR_s)\n",
        "print('\\nF1 Score: ',f1_score_s)\n",
        "\n",
        "#Model\n",
        "TP = n_cc+n_ss+n_rr\n",
        "FP = n_cs+n_cr+n_sc+n_sr+n_rc+n_rs\n",
        "FN = n_sc+n_rc+n_cs+n_rs+n_cr+n_sr\n",
        "\n",
        "precision = TP/(TP+FP)\n",
        "recall = TP/(TP+FN)\n",
        "f1_score = (2*precision*recall)/(precision+recall)\n",
        "\n",
        "print('----------------------------')\n",
        "print('\\n For Model Detection Over Coast:\\n')\n",
        "print('Precision: ',precision)\n",
        "print('Recall (TPR): ',recall)\n",
        "print('\\nF1 Score: ',f1_score)"
      ],
      "id": "yaj_FClId0L9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFW4ZG2sd0L9",
        "outputId": "0757f72b-860c-43e1-f1f2-0e0ad519f812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84/84 [==============================] - 0s 404us/step - loss: 0.1504 - recall: 0.9580\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.150408536195755, 0.9580209851264954]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_detection.evaluate(X_tst_detection, t_tst_detection, batch_size = batch_size1)"
      ],
      "id": "uFW4ZG2sd0L9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMSc6rlMd0L-"
      },
      "source": [
        "## **2.2 - Estimation networks (e-DNN)**"
      ],
      "id": "ZMSc6rlMd0L-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ckSWpR2d0L-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return backend.sqrt(backend.mean(backend.square(y_pred-y_true)))\n",
        "    \n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "        return backend.mean(backend.abs(y_pred-y_true))        "
      ],
      "id": "4ckSWpR2d0L-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp3t5zzmd0L-"
      },
      "source": [
        "### **2.2.1 Snowfall retrieval**"
      ],
      "id": "bp3t5zzmd0L-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaC51vchd0L-"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "hidden_units = 60\n",
        "dropout = 0\n",
        "\n",
        "# LAND\n",
        "model_retrieval = Sequential()\n",
        "\n",
        "model_retrieval.add(Dense(hidden_units))\n",
        "model_retrieval.add(Activation('relu'))\n",
        "model_retrieval.add(Dropout(dropout))\n",
        "\n",
        "model_retrieval.add(Dense(hidden_units))\n",
        "model_retrieval.add(Activation('relu'))\n",
        "model_retrieval.add(Dropout(dropout))\n",
        "\n",
        "model_retrieval.add(Dense(hidden_units))\n",
        "model_retrieval.add(Activation('relu'))\n",
        "model_retrieval.add(Dropout(dropout))\n",
        "\n",
        "model_retrieval.add(Dense(hidden_units))\n",
        "model_retrieval.add(Activation('relu'))\n",
        "model_retrieval.add(Dropout(dropout))\n",
        "\n",
        "model_retrieval.add(Dense(hidden_units))\n",
        "model_retrieval.add(Activation('relu'))\n",
        "model_retrieval.add(Dropout(dropout))\n",
        "\n",
        "model_retrieval.add(Dense(hidden_units))\n",
        "model_retrieval.add(Activation('relu'))\n",
        "model_retrieval.add(Dropout(dropout))\n",
        "\n",
        "model_retrieval.add(Dense(1))\n",
        "model_retrieval.add(Activation('relu'))"
      ],
      "id": "HaC51vchd0L-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMZNRdSdd0L_"
      },
      "outputs": [],
      "source": [
        "model_retrieval.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00001),\n",
        "              loss = root_mean_squared_error,\n",
        "              metrics= mean_absolute_error)"
      ],
      "id": "ZMZNRdSdd0L_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz48Epc6d0L_"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "callbacks_list = [\n",
        "#     keras.callbacks.EarlyStopping(\n",
        "#     monitor=\"val_loss\",\n",
        "#     patience=20,),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"checkpoint_path.keras\",\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True,\n",
        "    )    \n",
        "]"
      ],
      "id": "wz48Epc6d0L_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygjpfJ8Xd0L_",
        "outputId": "ea87d2bf-b3db-4fc8-a24a-3825468241a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fitting DNN (Retrieval Module - Snow):\n",
            "\n",
            "Epoch 1/300\n",
            "156/156 [==============================] - 13s 24ms/step - loss: 0.2505 - mean_absolute_error: 0.2092 - val_loss: 0.2028 - val_mean_absolute_error: 0.1671\n",
            "Epoch 2/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.2004 - mean_absolute_error: 0.1471 - val_loss: 0.1626 - val_mean_absolute_error: 0.1154\n",
            "Epoch 3/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.1723 - mean_absolute_error: 0.1069 - val_loss: 0.1403 - val_mean_absolute_error: 0.0828\n",
            "Epoch 4/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_absolute_error: 0.0824 - val_loss: 0.1286 - val_mean_absolute_error: 0.0655\n",
            "Epoch 5/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.1477 - mean_absolute_error: 0.0702 - val_loss: 0.1218 - val_mean_absolute_error: 0.0558\n",
            "Epoch 6/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.1417 - mean_absolute_error: 0.0630 - val_loss: 0.1176 - val_mean_absolute_error: 0.0517\n",
            "Epoch 7/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.1373 - mean_absolute_error: 0.0594 - val_loss: 0.1141 - val_mean_absolute_error: 0.0488\n",
            "Epoch 8/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.1336 - mean_absolute_error: 0.0568 - val_loss: 0.1113 - val_mean_absolute_error: 0.0467\n",
            "Epoch 9/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.1295 - mean_absolute_error: 0.0547 - val_loss: 0.1089 - val_mean_absolute_error: 0.0451\n",
            "Epoch 10/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.1274 - mean_absolute_error: 0.0531 - val_loss: 0.1068 - val_mean_absolute_error: 0.0440\n",
            "Epoch 11/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.1252 - mean_absolute_error: 0.0515 - val_loss: 0.1050 - val_mean_absolute_error: 0.0430\n",
            "Epoch 12/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.1225 - mean_absolute_error: 0.0503 - val_loss: 0.1032 - val_mean_absolute_error: 0.0420\n",
            "Epoch 13/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.1197 - mean_absolute_error: 0.0494 - val_loss: 0.1015 - val_mean_absolute_error: 0.0411\n",
            "Epoch 14/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.1186 - mean_absolute_error: 0.0480 - val_loss: 0.0999 - val_mean_absolute_error: 0.0403\n",
            "Epoch 15/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.1160 - mean_absolute_error: 0.0471 - val_loss: 0.0983 - val_mean_absolute_error: 0.0392\n",
            "Epoch 16/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.1138 - mean_absolute_error: 0.0458 - val_loss: 0.0970 - val_mean_absolute_error: 0.0387\n",
            "Epoch 17/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 0.1114 - mean_absolute_error: 0.0448 - val_loss: 0.0955 - val_mean_absolute_error: 0.0377\n",
            "Epoch 18/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.1096 - mean_absolute_error: 0.0439 - val_loss: 0.0943 - val_mean_absolute_error: 0.0369\n",
            "Epoch 19/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.1071 - mean_absolute_error: 0.0430 - val_loss: 0.0931 - val_mean_absolute_error: 0.0362\n",
            "Epoch 20/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.1074 - mean_absolute_error: 0.0421 - val_loss: 0.0921 - val_mean_absolute_error: 0.0357\n",
            "Epoch 21/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.1058 - mean_absolute_error: 0.0412 - val_loss: 0.0911 - val_mean_absolute_error: 0.0352\n",
            "Epoch 22/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.1047 - mean_absolute_error: 0.0405 - val_loss: 0.0902 - val_mean_absolute_error: 0.0348\n",
            "Epoch 23/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.1021 - mean_absolute_error: 0.0399 - val_loss: 0.0893 - val_mean_absolute_error: 0.0341\n",
            "Epoch 24/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.1014 - mean_absolute_error: 0.0391 - val_loss: 0.0885 - val_mean_absolute_error: 0.0336\n",
            "Epoch 25/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0999 - mean_absolute_error: 0.0387 - val_loss: 0.0878 - val_mean_absolute_error: 0.0331\n",
            "Epoch 26/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0999 - mean_absolute_error: 0.0381 - val_loss: 0.0872 - val_mean_absolute_error: 0.0327\n",
            "Epoch 27/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0991 - mean_absolute_error: 0.0375 - val_loss: 0.0865 - val_mean_absolute_error: 0.0323\n",
            "Epoch 28/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0975 - mean_absolute_error: 0.0370 - val_loss: 0.0859 - val_mean_absolute_error: 0.0319\n",
            "Epoch 29/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0968 - mean_absolute_error: 0.0365 - val_loss: 0.0853 - val_mean_absolute_error: 0.0317\n",
            "Epoch 30/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0948 - mean_absolute_error: 0.0362 - val_loss: 0.0849 - val_mean_absolute_error: 0.0314\n",
            "Epoch 31/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0957 - mean_absolute_error: 0.0356 - val_loss: 0.0843 - val_mean_absolute_error: 0.0311\n",
            "Epoch 32/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0935 - mean_absolute_error: 0.0353 - val_loss: 0.0839 - val_mean_absolute_error: 0.0308\n",
            "Epoch 33/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0933 - mean_absolute_error: 0.0350 - val_loss: 0.0836 - val_mean_absolute_error: 0.0306\n",
            "Epoch 34/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0931 - mean_absolute_error: 0.0346 - val_loss: 0.0832 - val_mean_absolute_error: 0.0302\n",
            "Epoch 35/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0922 - mean_absolute_error: 0.0341 - val_loss: 0.0828 - val_mean_absolute_error: 0.0301\n",
            "Epoch 36/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0922 - mean_absolute_error: 0.0339 - val_loss: 0.0826 - val_mean_absolute_error: 0.0300\n",
            "Epoch 37/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0904 - mean_absolute_error: 0.0336 - val_loss: 0.0822 - val_mean_absolute_error: 0.0298\n",
            "Epoch 38/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0896 - mean_absolute_error: 0.0333 - val_loss: 0.0819 - val_mean_absolute_error: 0.0296\n",
            "Epoch 39/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0893 - mean_absolute_error: 0.0329 - val_loss: 0.0818 - val_mean_absolute_error: 0.0296\n",
            "Epoch 40/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0887 - mean_absolute_error: 0.0326 - val_loss: 0.0816 - val_mean_absolute_error: 0.0294\n",
            "Epoch 41/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0885 - mean_absolute_error: 0.0324 - val_loss: 0.0811 - val_mean_absolute_error: 0.0292\n",
            "Epoch 42/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0876 - mean_absolute_error: 0.0320 - val_loss: 0.0810 - val_mean_absolute_error: 0.0292\n",
            "Epoch 43/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0866 - mean_absolute_error: 0.0320 - val_loss: 0.0807 - val_mean_absolute_error: 0.0290\n",
            "Epoch 44/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0873 - mean_absolute_error: 0.0317 - val_loss: 0.0805 - val_mean_absolute_error: 0.0291\n",
            "Epoch 45/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0863 - mean_absolute_error: 0.0316 - val_loss: 0.0802 - val_mean_absolute_error: 0.0287\n",
            "Epoch 46/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0866 - mean_absolute_error: 0.0313 - val_loss: 0.0800 - val_mean_absolute_error: 0.0288\n",
            "Epoch 47/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0859 - mean_absolute_error: 0.0311 - val_loss: 0.0798 - val_mean_absolute_error: 0.0288\n",
            "Epoch 48/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0842 - mean_absolute_error: 0.0309 - val_loss: 0.0796 - val_mean_absolute_error: 0.0288\n",
            "Epoch 49/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0845 - mean_absolute_error: 0.0308 - val_loss: 0.0794 - val_mean_absolute_error: 0.0285\n",
            "Epoch 50/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0837 - mean_absolute_error: 0.0306 - val_loss: 0.0792 - val_mean_absolute_error: 0.0282\n",
            "Epoch 51/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0838 - mean_absolute_error: 0.0303 - val_loss: 0.0789 - val_mean_absolute_error: 0.0283\n",
            "Epoch 52/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0829 - mean_absolute_error: 0.0300 - val_loss: 0.0786 - val_mean_absolute_error: 0.0282\n",
            "Epoch 53/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0826 - mean_absolute_error: 0.0300 - val_loss: 0.0786 - val_mean_absolute_error: 0.0281\n",
            "Epoch 54/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0814 - mean_absolute_error: 0.0297 - val_loss: 0.0783 - val_mean_absolute_error: 0.0280\n",
            "Epoch 55/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0808 - mean_absolute_error: 0.0295 - val_loss: 0.0782 - val_mean_absolute_error: 0.0279\n",
            "Epoch 56/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0807 - mean_absolute_error: 0.0294 - val_loss: 0.0780 - val_mean_absolute_error: 0.0280\n",
            "Epoch 57/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0811 - mean_absolute_error: 0.0292 - val_loss: 0.0779 - val_mean_absolute_error: 0.0278\n",
            "Epoch 58/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0799 - mean_absolute_error: 0.0290 - val_loss: 0.0776 - val_mean_absolute_error: 0.0279\n",
            "Epoch 59/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0807 - mean_absolute_error: 0.0290 - val_loss: 0.0775 - val_mean_absolute_error: 0.0277\n",
            "Epoch 60/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0807 - mean_absolute_error: 0.0288 - val_loss: 0.0774 - val_mean_absolute_error: 0.0275\n",
            "Epoch 61/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.0802 - mean_absolute_error: 0.0286 - val_loss: 0.0771 - val_mean_absolute_error: 0.0275\n",
            "Epoch 62/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0800 - mean_absolute_error: 0.0285 - val_loss: 0.0771 - val_mean_absolute_error: 0.0273\n",
            "Epoch 63/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0791 - mean_absolute_error: 0.0283 - val_loss: 0.0767 - val_mean_absolute_error: 0.0273\n",
            "Epoch 64/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0786 - mean_absolute_error: 0.0282 - val_loss: 0.0767 - val_mean_absolute_error: 0.0271\n",
            "Epoch 65/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0801 - mean_absolute_error: 0.0280 - val_loss: 0.0766 - val_mean_absolute_error: 0.0272\n",
            "Epoch 66/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0789 - mean_absolute_error: 0.0279 - val_loss: 0.0763 - val_mean_absolute_error: 0.0272\n",
            "Epoch 67/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0775 - mean_absolute_error: 0.0278 - val_loss: 0.0766 - val_mean_absolute_error: 0.0270\n",
            "Epoch 68/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0779 - mean_absolute_error: 0.0277 - val_loss: 0.0760 - val_mean_absolute_error: 0.0270\n",
            "Epoch 69/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.0780 - mean_absolute_error: 0.0275 - val_loss: 0.0759 - val_mean_absolute_error: 0.0269\n",
            "Epoch 70/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0761 - mean_absolute_error: 0.0275 - val_loss: 0.0757 - val_mean_absolute_error: 0.0270\n",
            "Epoch 71/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0775 - mean_absolute_error: 0.0273 - val_loss: 0.0755 - val_mean_absolute_error: 0.0270\n",
            "Epoch 72/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0766 - mean_absolute_error: 0.0272 - val_loss: 0.0756 - val_mean_absolute_error: 0.0267\n",
            "Epoch 73/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0774 - mean_absolute_error: 0.0271 - val_loss: 0.0753 - val_mean_absolute_error: 0.0267\n",
            "Epoch 74/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0764 - mean_absolute_error: 0.0270 - val_loss: 0.0750 - val_mean_absolute_error: 0.0267\n",
            "Epoch 75/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0763 - mean_absolute_error: 0.0270 - val_loss: 0.0749 - val_mean_absolute_error: 0.0268\n",
            "Epoch 76/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0761 - mean_absolute_error: 0.0268 - val_loss: 0.0748 - val_mean_absolute_error: 0.0265\n",
            "Epoch 77/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0761 - mean_absolute_error: 0.0268 - val_loss: 0.0747 - val_mean_absolute_error: 0.0264\n",
            "Epoch 78/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0755 - mean_absolute_error: 0.0266 - val_loss: 0.0743 - val_mean_absolute_error: 0.0264\n",
            "Epoch 79/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0756 - mean_absolute_error: 0.0265 - val_loss: 0.0743 - val_mean_absolute_error: 0.0264\n",
            "Epoch 80/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0747 - mean_absolute_error: 0.0263 - val_loss: 0.0741 - val_mean_absolute_error: 0.0264\n",
            "Epoch 81/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0745 - mean_absolute_error: 0.0263 - val_loss: 0.0741 - val_mean_absolute_error: 0.0262\n",
            "Epoch 82/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0751 - mean_absolute_error: 0.0262 - val_loss: 0.0738 - val_mean_absolute_error: 0.0263\n",
            "Epoch 83/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0734 - mean_absolute_error: 0.0261 - val_loss: 0.0737 - val_mean_absolute_error: 0.0264\n",
            "Epoch 84/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0735 - mean_absolute_error: 0.0260 - val_loss: 0.0736 - val_mean_absolute_error: 0.0262\n",
            "Epoch 85/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0740 - mean_absolute_error: 0.0260 - val_loss: 0.0734 - val_mean_absolute_error: 0.0261\n",
            "Epoch 86/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0751 - mean_absolute_error: 0.0259 - val_loss: 0.0734 - val_mean_absolute_error: 0.0261\n",
            "Epoch 87/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0738 - mean_absolute_error: 0.0257 - val_loss: 0.0734 - val_mean_absolute_error: 0.0260\n",
            "Epoch 88/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0747 - mean_absolute_error: 0.0257 - val_loss: 0.0733 - val_mean_absolute_error: 0.0259\n",
            "Epoch 89/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0734 - mean_absolute_error: 0.0256 - val_loss: 0.0731 - val_mean_absolute_error: 0.0259\n",
            "Epoch 90/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0729 - mean_absolute_error: 0.0255 - val_loss: 0.0732 - val_mean_absolute_error: 0.0257\n",
            "Epoch 91/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0741 - mean_absolute_error: 0.0254 - val_loss: 0.0728 - val_mean_absolute_error: 0.0257\n",
            "Epoch 92/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.0722 - mean_absolute_error: 0.0253 - val_loss: 0.0728 - val_mean_absolute_error: 0.0257\n",
            "Epoch 93/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0729 - mean_absolute_error: 0.0253 - val_loss: 0.0726 - val_mean_absolute_error: 0.0258\n",
            "Epoch 94/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0736 - mean_absolute_error: 0.0253 - val_loss: 0.0726 - val_mean_absolute_error: 0.0257\n",
            "Epoch 95/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0720 - mean_absolute_error: 0.0251 - val_loss: 0.0725 - val_mean_absolute_error: 0.0255\n",
            "Epoch 96/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0724 - mean_absolute_error: 0.0250 - val_loss: 0.0724 - val_mean_absolute_error: 0.0255\n",
            "Epoch 97/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0711 - mean_absolute_error: 0.0249 - val_loss: 0.0723 - val_mean_absolute_error: 0.0254\n",
            "Epoch 98/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0719 - mean_absolute_error: 0.0249 - val_loss: 0.0720 - val_mean_absolute_error: 0.0254\n",
            "Epoch 99/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0716 - mean_absolute_error: 0.0247 - val_loss: 0.0720 - val_mean_absolute_error: 0.0254\n",
            "Epoch 100/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0724 - mean_absolute_error: 0.0247 - val_loss: 0.0720 - val_mean_absolute_error: 0.0253\n",
            "Epoch 101/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0711 - mean_absolute_error: 0.0247 - val_loss: 0.0719 - val_mean_absolute_error: 0.0252\n",
            "Epoch 102/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0712 - mean_absolute_error: 0.0245 - val_loss: 0.0717 - val_mean_absolute_error: 0.0253\n",
            "Epoch 103/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0712 - mean_absolute_error: 0.0246 - val_loss: 0.0720 - val_mean_absolute_error: 0.0252\n",
            "Epoch 104/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0719 - mean_absolute_error: 0.0245 - val_loss: 0.0717 - val_mean_absolute_error: 0.0252\n",
            "Epoch 105/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0710 - mean_absolute_error: 0.0245 - val_loss: 0.0716 - val_mean_absolute_error: 0.0253\n",
            "Epoch 106/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0709 - mean_absolute_error: 0.0245 - val_loss: 0.0716 - val_mean_absolute_error: 0.0251\n",
            "Epoch 107/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0697 - mean_absolute_error: 0.0243 - val_loss: 0.0714 - val_mean_absolute_error: 0.0251\n",
            "Epoch 108/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0697 - mean_absolute_error: 0.0242 - val_loss: 0.0713 - val_mean_absolute_error: 0.0249\n",
            "Epoch 109/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0697 - mean_absolute_error: 0.0241 - val_loss: 0.0712 - val_mean_absolute_error: 0.0250\n",
            "Epoch 110/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0699 - mean_absolute_error: 0.0241 - val_loss: 0.0711 - val_mean_absolute_error: 0.0250\n",
            "Epoch 111/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0708 - mean_absolute_error: 0.0241 - val_loss: 0.0711 - val_mean_absolute_error: 0.0249\n",
            "Epoch 112/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0701 - mean_absolute_error: 0.0240 - val_loss: 0.0711 - val_mean_absolute_error: 0.0250\n",
            "Epoch 113/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0698 - mean_absolute_error: 0.0238 - val_loss: 0.0709 - val_mean_absolute_error: 0.0248\n",
            "Epoch 114/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0698 - mean_absolute_error: 0.0239 - val_loss: 0.0709 - val_mean_absolute_error: 0.0249\n",
            "Epoch 115/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0701 - mean_absolute_error: 0.0238 - val_loss: 0.0711 - val_mean_absolute_error: 0.0248\n",
            "Epoch 116/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0691 - mean_absolute_error: 0.0238 - val_loss: 0.0709 - val_mean_absolute_error: 0.0248\n",
            "Epoch 117/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0687 - mean_absolute_error: 0.0236 - val_loss: 0.0713 - val_mean_absolute_error: 0.0247\n",
            "Epoch 118/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0694 - mean_absolute_error: 0.0236 - val_loss: 0.0711 - val_mean_absolute_error: 0.0247\n",
            "Epoch 119/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0692 - mean_absolute_error: 0.0235 - val_loss: 0.0709 - val_mean_absolute_error: 0.0245\n",
            "Epoch 120/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0688 - mean_absolute_error: 0.0235 - val_loss: 0.0710 - val_mean_absolute_error: 0.0246\n",
            "Epoch 121/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0690 - mean_absolute_error: 0.0235 - val_loss: 0.0706 - val_mean_absolute_error: 0.0246\n",
            "Epoch 122/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0692 - mean_absolute_error: 0.0234 - val_loss: 0.0707 - val_mean_absolute_error: 0.0246\n",
            "Epoch 123/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0682 - mean_absolute_error: 0.0234 - val_loss: 0.0705 - val_mean_absolute_error: 0.0245\n",
            "Epoch 124/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0676 - mean_absolute_error: 0.0233 - val_loss: 0.0704 - val_mean_absolute_error: 0.0245\n",
            "Epoch 125/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0690 - mean_absolute_error: 0.0233 - val_loss: 0.0704 - val_mean_absolute_error: 0.0245\n",
            "Epoch 126/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0679 - mean_absolute_error: 0.0232 - val_loss: 0.0704 - val_mean_absolute_error: 0.0245\n",
            "Epoch 127/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0683 - mean_absolute_error: 0.0231 - val_loss: 0.0702 - val_mean_absolute_error: 0.0244\n",
            "Epoch 128/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0674 - mean_absolute_error: 0.0231 - val_loss: 0.0702 - val_mean_absolute_error: 0.0243\n",
            "Epoch 129/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0677 - mean_absolute_error: 0.0230 - val_loss: 0.0709 - val_mean_absolute_error: 0.0244\n",
            "Epoch 130/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0676 - mean_absolute_error: 0.0230 - val_loss: 0.0702 - val_mean_absolute_error: 0.0243\n",
            "Epoch 131/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0687 - mean_absolute_error: 0.0229 - val_loss: 0.0703 - val_mean_absolute_error: 0.0243\n",
            "Epoch 132/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0675 - mean_absolute_error: 0.0231 - val_loss: 0.0704 - val_mean_absolute_error: 0.0242\n",
            "Epoch 133/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0668 - mean_absolute_error: 0.0229 - val_loss: 0.0702 - val_mean_absolute_error: 0.0241\n",
            "Epoch 134/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0682 - mean_absolute_error: 0.0228 - val_loss: 0.0701 - val_mean_absolute_error: 0.0242\n",
            "Epoch 135/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0682 - mean_absolute_error: 0.0228 - val_loss: 0.0701 - val_mean_absolute_error: 0.0241\n",
            "Epoch 136/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0670 - mean_absolute_error: 0.0228 - val_loss: 0.0701 - val_mean_absolute_error: 0.0241\n",
            "Epoch 137/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0669 - mean_absolute_error: 0.0227 - val_loss: 0.0701 - val_mean_absolute_error: 0.0241\n",
            "Epoch 138/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0666 - mean_absolute_error: 0.0227 - val_loss: 0.0701 - val_mean_absolute_error: 0.0241\n",
            "Epoch 139/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0661 - mean_absolute_error: 0.0226 - val_loss: 0.0702 - val_mean_absolute_error: 0.0240\n",
            "Epoch 140/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0671 - mean_absolute_error: 0.0226 - val_loss: 0.0702 - val_mean_absolute_error: 0.0240\n",
            "Epoch 141/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0667 - mean_absolute_error: 0.0226 - val_loss: 0.0702 - val_mean_absolute_error: 0.0240\n",
            "Epoch 142/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0673 - mean_absolute_error: 0.0225 - val_loss: 0.0701 - val_mean_absolute_error: 0.0239\n",
            "Epoch 143/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0664 - mean_absolute_error: 0.0225 - val_loss: 0.0703 - val_mean_absolute_error: 0.0240\n",
            "Epoch 144/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0658 - mean_absolute_error: 0.0224 - val_loss: 0.0700 - val_mean_absolute_error: 0.0240\n",
            "Epoch 145/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0661 - mean_absolute_error: 0.0223 - val_loss: 0.0699 - val_mean_absolute_error: 0.0240\n",
            "Epoch 146/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0669 - mean_absolute_error: 0.0224 - val_loss: 0.0701 - val_mean_absolute_error: 0.0240\n",
            "Epoch 147/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0669 - mean_absolute_error: 0.0224 - val_loss: 0.0701 - val_mean_absolute_error: 0.0238\n",
            "Epoch 148/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0661 - mean_absolute_error: 0.0223 - val_loss: 0.0699 - val_mean_absolute_error: 0.0238\n",
            "Epoch 149/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0652 - mean_absolute_error: 0.0222 - val_loss: 0.0701 - val_mean_absolute_error: 0.0241\n",
            "Epoch 150/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0661 - mean_absolute_error: 0.0222 - val_loss: 0.0701 - val_mean_absolute_error: 0.0238\n",
            "Epoch 151/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0665 - mean_absolute_error: 0.0222 - val_loss: 0.0701 - val_mean_absolute_error: 0.0238\n",
            "Epoch 152/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0663 - mean_absolute_error: 0.0221 - val_loss: 0.0699 - val_mean_absolute_error: 0.0238\n",
            "Epoch 153/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0650 - mean_absolute_error: 0.0220 - val_loss: 0.0700 - val_mean_absolute_error: 0.0237\n",
            "Epoch 154/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0655 - mean_absolute_error: 0.0221 - val_loss: 0.0699 - val_mean_absolute_error: 0.0238\n",
            "Epoch 155/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0664 - mean_absolute_error: 0.0220 - val_loss: 0.0698 - val_mean_absolute_error: 0.0237\n",
            "Epoch 156/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0653 - mean_absolute_error: 0.0221 - val_loss: 0.0698 - val_mean_absolute_error: 0.0236\n",
            "Epoch 157/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0647 - mean_absolute_error: 0.0220 - val_loss: 0.0701 - val_mean_absolute_error: 0.0237\n",
            "Epoch 158/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0641 - mean_absolute_error: 0.0220 - val_loss: 0.0697 - val_mean_absolute_error: 0.0237\n",
            "Epoch 159/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0645 - mean_absolute_error: 0.0218 - val_loss: 0.0699 - val_mean_absolute_error: 0.0237\n",
            "Epoch 160/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0654 - mean_absolute_error: 0.0219 - val_loss: 0.0697 - val_mean_absolute_error: 0.0238\n",
            "Epoch 161/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0645 - mean_absolute_error: 0.0218 - val_loss: 0.0698 - val_mean_absolute_error: 0.0237\n",
            "Epoch 162/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0647 - mean_absolute_error: 0.0218 - val_loss: 0.0699 - val_mean_absolute_error: 0.0237\n",
            "Epoch 163/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0648 - mean_absolute_error: 0.0217 - val_loss: 0.0698 - val_mean_absolute_error: 0.0236\n",
            "Epoch 164/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0640 - mean_absolute_error: 0.0217 - val_loss: 0.0695 - val_mean_absolute_error: 0.0236\n",
            "Epoch 165/300\n",
            "156/156 [==============================] - ETA: 0s - loss: 0.0640 - mean_absolute_error: 0.021 - 1s 4ms/step - loss: 0.0640 - mean_absolute_error: 0.0217 - val_loss: 0.0700 - val_mean_absolute_error: 0.0236\n",
            "Epoch 166/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0638 - mean_absolute_error: 0.0217 - val_loss: 0.0695 - val_mean_absolute_error: 0.0236\n",
            "Epoch 167/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0639 - mean_absolute_error: 0.0216 - val_loss: 0.0693 - val_mean_absolute_error: 0.0235\n",
            "Epoch 168/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0642 - mean_absolute_error: 0.0216 - val_loss: 0.0695 - val_mean_absolute_error: 0.0234\n",
            "Epoch 169/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0642 - mean_absolute_error: 0.0216 - val_loss: 0.0695 - val_mean_absolute_error: 0.0234\n",
            "Epoch 170/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0643 - mean_absolute_error: 0.0215 - val_loss: 0.0696 - val_mean_absolute_error: 0.0236\n",
            "Epoch 171/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0641 - mean_absolute_error: 0.0215 - val_loss: 0.0699 - val_mean_absolute_error: 0.0235\n",
            "Epoch 172/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0631 - mean_absolute_error: 0.0214 - val_loss: 0.0695 - val_mean_absolute_error: 0.0234\n",
            "Epoch 173/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0646 - mean_absolute_error: 0.0214 - val_loss: 0.0696 - val_mean_absolute_error: 0.0234\n",
            "Epoch 174/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0638 - mean_absolute_error: 0.0215 - val_loss: 0.0696 - val_mean_absolute_error: 0.0234\n",
            "Epoch 175/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0633 - mean_absolute_error: 0.0214 - val_loss: 0.0700 - val_mean_absolute_error: 0.0234\n",
            "Epoch 176/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0644 - mean_absolute_error: 0.0213 - val_loss: 0.0696 - val_mean_absolute_error: 0.0235\n",
            "Epoch 177/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0634 - mean_absolute_error: 0.0213 - val_loss: 0.0695 - val_mean_absolute_error: 0.0234\n",
            "Epoch 178/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0634 - mean_absolute_error: 0.0213 - val_loss: 0.0696 - val_mean_absolute_error: 0.0234\n",
            "Epoch 179/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0645 - mean_absolute_error: 0.0213 - val_loss: 0.0695 - val_mean_absolute_error: 0.0233\n",
            "Epoch 180/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0628 - mean_absolute_error: 0.0212 - val_loss: 0.0695 - val_mean_absolute_error: 0.0233\n",
            "Epoch 181/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0629 - mean_absolute_error: 0.0212 - val_loss: 0.0695 - val_mean_absolute_error: 0.0233\n",
            "Epoch 182/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.0637 - mean_absolute_error: 0.0211 - val_loss: 0.0695 - val_mean_absolute_error: 0.0234\n",
            "Epoch 183/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0639 - mean_absolute_error: 0.0212 - val_loss: 0.0694 - val_mean_absolute_error: 0.0234\n",
            "Epoch 184/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0628 - mean_absolute_error: 0.0211 - val_loss: 0.0697 - val_mean_absolute_error: 0.0233\n",
            "Epoch 185/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0629 - mean_absolute_error: 0.0210 - val_loss: 0.0695 - val_mean_absolute_error: 0.0233\n",
            "Epoch 186/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0626 - mean_absolute_error: 0.0211 - val_loss: 0.0694 - val_mean_absolute_error: 0.0232\n",
            "Epoch 187/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0631 - mean_absolute_error: 0.0210 - val_loss: 0.0696 - val_mean_absolute_error: 0.0232\n",
            "Epoch 188/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0627 - mean_absolute_error: 0.0210 - val_loss: 0.0698 - val_mean_absolute_error: 0.0233\n",
            "Epoch 189/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0629 - mean_absolute_error: 0.0209 - val_loss: 0.0696 - val_mean_absolute_error: 0.0233\n",
            "Epoch 190/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0615 - mean_absolute_error: 0.0210 - val_loss: 0.0695 - val_mean_absolute_error: 0.0233\n",
            "Epoch 191/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0631 - mean_absolute_error: 0.0209 - val_loss: 0.0695 - val_mean_absolute_error: 0.0233\n",
            "Epoch 192/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0622 - mean_absolute_error: 0.0208 - val_loss: 0.0696 - val_mean_absolute_error: 0.0232\n",
            "Epoch 193/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0630 - mean_absolute_error: 0.0209 - val_loss: 0.0695 - val_mean_absolute_error: 0.0232\n",
            "Epoch 194/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0620 - mean_absolute_error: 0.0208 - val_loss: 0.0693 - val_mean_absolute_error: 0.0232\n",
            "Epoch 195/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0624 - mean_absolute_error: 0.0207 - val_loss: 0.0695 - val_mean_absolute_error: 0.0232\n",
            "Epoch 196/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0627 - mean_absolute_error: 0.0207 - val_loss: 0.0695 - val_mean_absolute_error: 0.0232\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 197/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0622 - mean_absolute_error: 0.0207 - val_loss: 0.0696 - val_mean_absolute_error: 0.0232\n",
            "Epoch 198/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0621 - mean_absolute_error: 0.0207 - val_loss: 0.0700 - val_mean_absolute_error: 0.0232\n",
            "Epoch 199/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0620 - mean_absolute_error: 0.0207 - val_loss: 0.0694 - val_mean_absolute_error: 0.0233\n",
            "Epoch 200/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0621 - mean_absolute_error: 0.0207 - val_loss: 0.0697 - val_mean_absolute_error: 0.0232\n",
            "Epoch 201/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0616 - mean_absolute_error: 0.0206 - val_loss: 0.0695 - val_mean_absolute_error: 0.0231\n",
            "Epoch 202/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0617 - mean_absolute_error: 0.0206 - val_loss: 0.0695 - val_mean_absolute_error: 0.0231\n",
            "Epoch 203/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0626 - mean_absolute_error: 0.0205 - val_loss: 0.0695 - val_mean_absolute_error: 0.0230\n",
            "Epoch 204/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0615 - mean_absolute_error: 0.0206 - val_loss: 0.0696 - val_mean_absolute_error: 0.0231\n",
            "Epoch 205/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0625 - mean_absolute_error: 0.0206 - val_loss: 0.0694 - val_mean_absolute_error: 0.0231\n",
            "Epoch 206/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0612 - mean_absolute_error: 0.0204 - val_loss: 0.0693 - val_mean_absolute_error: 0.0231\n",
            "Epoch 207/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.0602 - mean_absolute_error: 0.0204 - val_loss: 0.0696 - val_mean_absolute_error: 0.0231\n",
            "Epoch 208/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0620 - mean_absolute_error: 0.0205 - val_loss: 0.0694 - val_mean_absolute_error: 0.0232\n",
            "Epoch 209/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0617 - mean_absolute_error: 0.0205 - val_loss: 0.0691 - val_mean_absolute_error: 0.0231\n",
            "Epoch 210/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0620 - mean_absolute_error: 0.0205 - val_loss: 0.0695 - val_mean_absolute_error: 0.0231\n",
            "Epoch 211/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0612 - mean_absolute_error: 0.0203 - val_loss: 0.0691 - val_mean_absolute_error: 0.0230\n",
            "Epoch 212/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0610 - mean_absolute_error: 0.0203 - val_loss: 0.0692 - val_mean_absolute_error: 0.0232\n",
            "Epoch 213/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0621 - mean_absolute_error: 0.0204 - val_loss: 0.0698 - val_mean_absolute_error: 0.0231\n",
            "Epoch 214/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0614 - mean_absolute_error: 0.0203 - val_loss: 0.0692 - val_mean_absolute_error: 0.0231\n",
            "Epoch 215/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0618 - mean_absolute_error: 0.0203 - val_loss: 0.0694 - val_mean_absolute_error: 0.0230\n",
            "Epoch 216/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0609 - mean_absolute_error: 0.0203 - val_loss: 0.0693 - val_mean_absolute_error: 0.0230\n",
            "Epoch 217/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0613 - mean_absolute_error: 0.0202 - val_loss: 0.0695 - val_mean_absolute_error: 0.0231\n",
            "Epoch 218/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0607 - mean_absolute_error: 0.0202 - val_loss: 0.0695 - val_mean_absolute_error: 0.0230\n",
            "Epoch 219/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.0607 - mean_absolute_error: 0.0202 - val_loss: 0.0692 - val_mean_absolute_error: 0.0230\n",
            "Epoch 220/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0607 - mean_absolute_error: 0.0201 - val_loss: 0.0696 - val_mean_absolute_error: 0.0230\n",
            "Epoch 221/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0612 - mean_absolute_error: 0.0201 - val_loss: 0.0694 - val_mean_absolute_error: 0.0230\n",
            "Epoch 222/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0606 - mean_absolute_error: 0.0201 - val_loss: 0.0693 - val_mean_absolute_error: 0.0230\n",
            "Epoch 223/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0614 - mean_absolute_error: 0.0201 - val_loss: 0.0693 - val_mean_absolute_error: 0.0230\n",
            "Epoch 224/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0602 - mean_absolute_error: 0.0200 - val_loss: 0.0694 - val_mean_absolute_error: 0.0229\n",
            "Epoch 225/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0608 - mean_absolute_error: 0.0200 - val_loss: 0.0694 - val_mean_absolute_error: 0.0229\n",
            "Epoch 226/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0610 - mean_absolute_error: 0.0200 - val_loss: 0.0695 - val_mean_absolute_error: 0.0230\n",
            "Epoch 227/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0607 - mean_absolute_error: 0.0200 - val_loss: 0.0700 - val_mean_absolute_error: 0.0230\n",
            "Epoch 228/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0607 - mean_absolute_error: 0.0200 - val_loss: 0.0695 - val_mean_absolute_error: 0.0229\n",
            "Epoch 229/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0607 - mean_absolute_error: 0.0200 - val_loss: 0.0692 - val_mean_absolute_error: 0.0230\n",
            "Epoch 230/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0596 - mean_absolute_error: 0.0199 - val_loss: 0.0691 - val_mean_absolute_error: 0.0228\n",
            "Epoch 231/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0599 - mean_absolute_error: 0.0199 - val_loss: 0.0693 - val_mean_absolute_error: 0.0228\n",
            "Epoch 232/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0602 - mean_absolute_error: 0.0199 - val_loss: 0.0694 - val_mean_absolute_error: 0.0228\n",
            "Epoch 233/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.0596 - mean_absolute_error: 0.0199 - val_loss: 0.0693 - val_mean_absolute_error: 0.0228\n",
            "Epoch 234/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0598 - mean_absolute_error: 0.0198 - val_loss: 0.0697 - val_mean_absolute_error: 0.0229\n",
            "Epoch 235/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0593 - mean_absolute_error: 0.0198 - val_loss: 0.0695 - val_mean_absolute_error: 0.0228\n",
            "Epoch 236/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0605 - mean_absolute_error: 0.0198 - val_loss: 0.0692 - val_mean_absolute_error: 0.0228\n",
            "Epoch 237/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0606 - mean_absolute_error: 0.0198 - val_loss: 0.0692 - val_mean_absolute_error: 0.0228\n",
            "Epoch 238/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0599 - mean_absolute_error: 0.0198 - val_loss: 0.0692 - val_mean_absolute_error: 0.0228\n",
            "Epoch 239/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0591 - mean_absolute_error: 0.0197 - val_loss: 0.0692 - val_mean_absolute_error: 0.0229\n",
            "Epoch 240/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0598 - mean_absolute_error: 0.0197 - val_loss: 0.0693 - val_mean_absolute_error: 0.0227\n",
            "Epoch 241/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0600 - mean_absolute_error: 0.0197 - val_loss: 0.0692 - val_mean_absolute_error: 0.0227\n",
            "Epoch 242/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0599 - mean_absolute_error: 0.0197 - val_loss: 0.0691 - val_mean_absolute_error: 0.0227\n",
            "Epoch 243/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0584 - mean_absolute_error: 0.0196 - val_loss: 0.0692 - val_mean_absolute_error: 0.0228\n",
            "Epoch 244/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0592 - mean_absolute_error: 0.0196 - val_loss: 0.0691 - val_mean_absolute_error: 0.0228\n",
            "Epoch 245/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0601 - mean_absolute_error: 0.0196 - val_loss: 0.0691 - val_mean_absolute_error: 0.0228\n",
            "Epoch 246/300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0588 - mean_absolute_error: 0.0196 - val_loss: 0.0696 - val_mean_absolute_error: 0.0229\n",
            "Epoch 247/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0592 - mean_absolute_error: 0.0196 - val_loss: 0.0692 - val_mean_absolute_error: 0.0227\n",
            "Epoch 248/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0604 - mean_absolute_error: 0.0196 - val_loss: 0.0692 - val_mean_absolute_error: 0.0227\n",
            "Epoch 249/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0594 - mean_absolute_error: 0.0196 - val_loss: 0.0695 - val_mean_absolute_error: 0.0227\n",
            "Epoch 250/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0594 - mean_absolute_error: 0.0195 - val_loss: 0.0694 - val_mean_absolute_error: 0.0227\n",
            "Epoch 251/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0599 - mean_absolute_error: 0.0195 - val_loss: 0.0694 - val_mean_absolute_error: 0.0228\n",
            "Epoch 252/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0581 - mean_absolute_error: 0.0195 - val_loss: 0.0695 - val_mean_absolute_error: 0.0227\n",
            "Epoch 253/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0587 - mean_absolute_error: 0.0195 - val_loss: 0.0695 - val_mean_absolute_error: 0.0228\n",
            "Epoch 254/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0591 - mean_absolute_error: 0.0195 - val_loss: 0.0694 - val_mean_absolute_error: 0.0227\n",
            "Epoch 255/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0587 - mean_absolute_error: 0.0195 - val_loss: 0.0693 - val_mean_absolute_error: 0.0227\n",
            "Epoch 256/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0579 - mean_absolute_error: 0.0195 - val_loss: 0.0691 - val_mean_absolute_error: 0.0226\n",
            "Epoch 257/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0584 - mean_absolute_error: 0.0194 - val_loss: 0.0692 - val_mean_absolute_error: 0.0227\n",
            "Epoch 258/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0588 - mean_absolute_error: 0.0194 - val_loss: 0.0692 - val_mean_absolute_error: 0.0226\n",
            "Epoch 259/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0581 - mean_absolute_error: 0.0193 - val_loss: 0.0691 - val_mean_absolute_error: 0.0228\n",
            "Epoch 260/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0577 - mean_absolute_error: 0.0193 - val_loss: 0.0692 - val_mean_absolute_error: 0.0229\n",
            "Epoch 261/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0577 - mean_absolute_error: 0.0193 - val_loss: 0.0695 - val_mean_absolute_error: 0.0227\n",
            "Epoch 262/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0587 - mean_absolute_error: 0.0194 - val_loss: 0.0693 - val_mean_absolute_error: 0.0226\n",
            "Epoch 263/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0593 - mean_absolute_error: 0.0193 - val_loss: 0.0694 - val_mean_absolute_error: 0.0227\n",
            "Epoch 264/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0585 - mean_absolute_error: 0.0193 - val_loss: 0.0692 - val_mean_absolute_error: 0.0227\n",
            "Epoch 265/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0589 - mean_absolute_error: 0.0192 - val_loss: 0.0694 - val_mean_absolute_error: 0.0226\n",
            "Epoch 266/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0576 - mean_absolute_error: 0.0192 - val_loss: 0.0691 - val_mean_absolute_error: 0.0226\n",
            "Epoch 267/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0581 - mean_absolute_error: 0.0192 - val_loss: 0.0693 - val_mean_absolute_error: 0.0226\n",
            "Epoch 268/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0585 - mean_absolute_error: 0.0192 - val_loss: 0.0691 - val_mean_absolute_error: 0.0227\n",
            "Epoch 269/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0589 - mean_absolute_error: 0.0193 - val_loss: 0.0692 - val_mean_absolute_error: 0.0226\n",
            "Epoch 270/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0582 - mean_absolute_error: 0.0192 - val_loss: 0.0691 - val_mean_absolute_error: 0.0226\n",
            "Epoch 271/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0580 - mean_absolute_error: 0.0192 - val_loss: 0.0691 - val_mean_absolute_error: 0.0226\n",
            "Epoch 272/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0583 - mean_absolute_error: 0.0192 - val_loss: 0.0689 - val_mean_absolute_error: 0.0226\n",
            "Epoch 273/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0586 - mean_absolute_error: 0.0191 - val_loss: 0.0690 - val_mean_absolute_error: 0.0227\n",
            "Epoch 274/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0584 - mean_absolute_error: 0.0192 - val_loss: 0.0691 - val_mean_absolute_error: 0.0226\n",
            "Epoch 275/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0586 - mean_absolute_error: 0.0190 - val_loss: 0.0691 - val_mean_absolute_error: 0.0227\n",
            "Epoch 276/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0576 - mean_absolute_error: 0.0191 - val_loss: 0.0692 - val_mean_absolute_error: 0.0226\n",
            "Epoch 277/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0578 - mean_absolute_error: 0.0190 - val_loss: 0.0693 - val_mean_absolute_error: 0.0226\n",
            "Epoch 278/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0589 - mean_absolute_error: 0.0190 - val_loss: 0.0691 - val_mean_absolute_error: 0.0226\n",
            "Epoch 279/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0588 - mean_absolute_error: 0.0190 - val_loss: 0.0692 - val_mean_absolute_error: 0.0226\n",
            "Epoch 280/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0580 - mean_absolute_error: 0.0189 - val_loss: 0.0698 - val_mean_absolute_error: 0.0229\n",
            "Epoch 281/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0593 - mean_absolute_error: 0.0190 - val_loss: 0.0691 - val_mean_absolute_error: 0.0226\n",
            "Epoch 282/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0583 - mean_absolute_error: 0.0189 - val_loss: 0.0692 - val_mean_absolute_error: 0.0226\n",
            "Epoch 283/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0575 - mean_absolute_error: 0.0189 - val_loss: 0.0696 - val_mean_absolute_error: 0.0227\n",
            "Epoch 284/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0578 - mean_absolute_error: 0.0189 - val_loss: 0.0693 - val_mean_absolute_error: 0.0226\n",
            "Epoch 285/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0572 - mean_absolute_error: 0.0189 - val_loss: 0.0692 - val_mean_absolute_error: 0.0227\n",
            "Epoch 286/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0579 - mean_absolute_error: 0.0189 - val_loss: 0.0693 - val_mean_absolute_error: 0.0226\n",
            "Epoch 287/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0576 - mean_absolute_error: 0.0189 - val_loss: 0.0694 - val_mean_absolute_error: 0.0226\n",
            "Epoch 288/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0574 - mean_absolute_error: 0.0189 - val_loss: 0.0693 - val_mean_absolute_error: 0.0226\n",
            "Epoch 289/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.0577 - mean_absolute_error: 0.0189 - val_loss: 0.0691 - val_mean_absolute_error: 0.0226\n",
            "Epoch 290/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0575 - mean_absolute_error: 0.0189 - val_loss: 0.0694 - val_mean_absolute_error: 0.0226\n",
            "Epoch 291/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0580 - mean_absolute_error: 0.0189 - val_loss: 0.0692 - val_mean_absolute_error: 0.0226\n",
            "Epoch 292/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0571 - mean_absolute_error: 0.0188 - val_loss: 0.0694 - val_mean_absolute_error: 0.0226\n",
            "Epoch 293/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0575 - mean_absolute_error: 0.0188 - val_loss: 0.0691 - val_mean_absolute_error: 0.0227\n",
            "Epoch 294/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0583 - mean_absolute_error: 0.0188 - val_loss: 0.0693 - val_mean_absolute_error: 0.0228\n",
            "Epoch 295/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0565 - mean_absolute_error: 0.0188 - val_loss: 0.0693 - val_mean_absolute_error: 0.0226\n",
            "Epoch 296/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.0580 - mean_absolute_error: 0.0188 - val_loss: 0.0693 - val_mean_absolute_error: 0.0226\n",
            "Epoch 297/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0568 - mean_absolute_error: 0.0188 - val_loss: 0.0693 - val_mean_absolute_error: 0.0227\n",
            "Epoch 298/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0581 - mean_absolute_error: 0.0187 - val_loss: 0.0692 - val_mean_absolute_error: 0.0225\n",
            "Epoch 299/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0574 - mean_absolute_error: 0.0187 - val_loss: 0.0695 - val_mean_absolute_error: 0.0227\n",
            "Epoch 300/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.0571 - mean_absolute_error: 0.0187 - val_loss: 0.0695 - val_mean_absolute_error: 0.0226\n"
          ]
        }
      ],
      "source": [
        "print('\\nFitting DNN (Retrieval Module - Snow):\\n')\n",
        "batch_size2 = 40\n",
        "history_retrieval = model_retrieval.fit(X_trn_retrieval, y_trn_retrieval, epochs=300,\n",
        "                                validation_split=.2, batch_size = batch_size2,\n",
        "                                callbacks=callbacks_list, verbose=1)"
      ],
      "id": "ygjpfJ8Xd0L_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2587erXjd0L_",
        "outputId": "7a0291fe-99ac-4a34-fc6f-abb62ba4f724"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x201bf007cc8>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "val_loss = history_retrieval.history[\"val_loss\"]\n",
        "loss = history_retrieval.history[\"loss\"]\n",
        "\n",
        "epochs = range(1, 301)\n",
        "plt.plot(epochs, val_loss[:], \"r-\",\n",
        "label=\"Validation Loss\")\n",
        "plt.plot(epochs, loss[:], \"b-\",\n",
        "label=\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "id": "2587erXjd0L_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diu3oPxYd0L_"
      },
      "outputs": [],
      "source": [
        "predictions_snow = model_retrieval.predict(X_tst_retrieval)"
      ],
      "id": "diu3oPxYd0L_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-5afAs6d0MA"
      },
      "source": [
        "### Evaluation"
      ],
      "id": "r-5afAs6d0MA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIfiJYr9d0MA",
        "outputId": "3b9a7426-2ee3-417b-e3cb-27a38030751a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84/84 [==============================] - 0s 2ms/step - loss: 0.0964 - mean_absolute_error: 0.0346\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.09644481539726257, 0.03464727848768234]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_retrieval.evaluate(X_tst_retrieval, y_tst_retrieval, batch_size = batch_size2)"
      ],
      "id": "sIfiJYr9d0MA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66gIyB7jd0MA"
      },
      "source": [
        "# **3. Saving the models**"
      ],
      "id": "66gIyB7jd0MA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75YKws-kd0MB"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "from scipy.io import savemat\n",
        "\n",
        "model_detection.save('Models\\CPR\\Coast\\model_dtc',save_format='h5')\n",
        "model_retrieval.save('Models\\CPR\\Coast\\model_snow',save_format='h5')\n",
        "\n",
        "fp_CPR_coast = 'Models/CPR/coast/files_CPR_coast.mat'\n",
        "scipy.io.savemat(fp_CPR_coast, {'mean_detection_CPR_coast': mean_detection,'std_detection_CPR_coast':std_detection,\n",
        "                               'mean_snow_retrieval_CPR_coast': mean_retrieval,'std_snow_retrieval_CPR_coast':std_retrieval})"
      ],
      "id": "75YKws-kd0MB"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}